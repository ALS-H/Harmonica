{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e32eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\thecuda_env21stapr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Library Versions ---\n",
      "Transformers: 4.51.3\n",
      "PEFT: 0.15.2\n",
      "Accelerate: 1.6.0\n",
      "Datasets: 3.5.0\n",
      "Torch: 2.6.0+cu124\n",
      "Evaluate: 0.4.3\n",
      "Audiomentations: 2.2.3\n",
      "----------------------\n",
      "--- Imports successful ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 1: Imports\n",
    "# Import necessary libraries.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import evaluate # Hugging Face evaluation library\n",
    "import warnings\n",
    "import gc # Garbage collector\n",
    "import types # Import the types module for binding methods\n",
    "import traceback # For printing tracebacks\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Audio, Value\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    Wav2Vec2FeatureExtractor, # Explicit import\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Import for early stopping\n",
    ")\n",
    "# Import for Augmentation\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch\n",
    "\n",
    "# Import for PEFT (LoRA/QLoRA)\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# Suppress less critical warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print library versions (optional but good practice)\n",
    "import transformers\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "print(\"--- Library Versions ---\")\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Evaluate:\", evaluate.__version__)\n",
    "print(\"Audiomentations:\", pd.__version__) # Assuming you meant pandas or audiomentations here\n",
    "print(\"----------------------\")\n",
    "print(\"--- Imports successful ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d448b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuring paths and parameters ---\n",
      "RAVDESS Path: ./RAVDESS Emotional speech audio\n",
      "Base Model ID: superb/wav2vec2-base-superb-er\n",
      "Output Directory: ./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\n",
      "Target Labels: ['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "Number of Labels: 7\n",
      "Label2ID: {'angry': 0, 'disgust': 1, 'fearful': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprised': 6}\n",
      "ID2Label: {0: 'angry', 1: 'disgust', 2: 'fearful', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprised'}\n",
      "Number of Epochs: 15\n",
      "Learning Rate: 0.0001\n",
      "Batch Size: 4\n",
      "Gradient Accumulation: 4\n",
      "Effective Batch Size: 16\n",
      "Eval Strategy: epoch\n",
      "Load Best Model: True\n",
      "Metric for Best Model: eval_f1\n",
      "Early Stopping Patience: 5\n",
      "LoRA Parameters: r=32, alpha=64, dropout=0.05, target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj']\n",
      "--- Configuration loaded ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 2: Configuration\n",
    "# Define paths, model parameters, training hyperparameters, and LoRA settings.\n",
    "\n",
    "# %%\n",
    "print(\"--- Configuring paths and parameters ---\")\n",
    "\n",
    "# --- Data Path ---\n",
    "RAVDESS_DATA_PATH = \"./RAVDESS Emotional speech audio\" #<-- ADJUST IF YOUR PATH IS DIFFERENT\n",
    "\n",
    "# --- Model Configuration ---\n",
    "MODEL_ID = \"superb/wav2vec2-base-superb-er\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
    "# Give a new output directory name for this improved run using LORA\n",
    "FINETUNED_MODEL_OUTPUT_DIR = f\"./{MODEL_NAME}-finetuned-full-ravdess-v2-lora\" # <--- Added -lora\n",
    "\n",
    "# --- RAVDESS Specific Configuration ---\n",
    "RAVDESS_EMOTION_MAP = {\n",
    "    \"01\": \"neutral\",    # Neutral\n",
    "    \"02\": \"neutral\",    # Calm -> Mapped to Neutral\n",
    "    \"03\": \"happy\",      # Happy\n",
    "    \"04\": \"sad\",        # Sad\n",
    "    \"05\": \"angry\",      # Angry\n",
    "    \"06\": \"fearful\",    # Fearful\n",
    "    \"07\": \"disgust\",    # Disgust\n",
    "    \"08\": \"surprised\"   # Surprised\n",
    "}\n",
    "TARGET_LABELS = sorted(list(set(RAVDESS_EMOTION_MAP.values()))) # Get unique mapped labels\n",
    "TARGET_MODALITY = \"03\" # Audio-only\n",
    "TARGET_VOCAL_CHANNEL = \"01\" # Speech\n",
    "label2id = {label: i for i, label in enumerate(TARGET_LABELS)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "NUM_LABELS = len(TARGET_LABELS)\n",
    "\n",
    "# --- Training Configuration ---\n",
    "NUM_EPOCHS = 15 # Increase number of epochs again, LoRA might need more\n",
    "LEARNING_RATE = 1e-4 # Try a slightly higher LR for LoRA adapters (can be tuned)\n",
    "BATCH_SIZE = 4   # Keep or adjust based on your GPU memory\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # Keep this\n",
    "EVAL_STRATEGY = \"epoch\" # Evaluate each epoch\n",
    "SAVE_STRATEGY = \"epoch\" # Save each epoch\n",
    "LOGGING_STEPS = 50 # Log every 50 steps\n",
    "LOAD_BEST_MODEL_AT_END = True # Load the best model based on validation metric\n",
    "METRIC_FOR_BEST_MODEL = \"eval_f1\" # Use F1-score (requires 'f1' key from compute_metrics)\n",
    "GREATER_IS_BETTER = True # F1/Accuracy should be maximized\n",
    "SEED = 42\n",
    "EARLY_STOPPING_PATIENCE = 5 # Increase patience slightly\n",
    "SAVE_TOTAL_LIMIT = 3 # Keep more checkpoints if needed\n",
    "\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "LORA_R = 32 # LoRA attention dimension (rank)\n",
    "LORA_ALPHA = 64 # Alpha parameter for LoRA scaling\n",
    "LORA_DROPOUT = 0.05 # Dropout probability for LoRA layers\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"] # Common for attention layers\n",
    "\n",
    "# --- Print Configuration ---\n",
    "print(f\"RAVDESS Path: {RAVDESS_DATA_PATH}\")\n",
    "print(f\"Base Model ID: {MODEL_ID}\")\n",
    "print(f\"Output Directory: {FINETUNED_MODEL_OUTPUT_DIR}\")\n",
    "print(f\"Target Labels: {TARGET_LABELS}\")\n",
    "print(f\"Number of Labels: {NUM_LABELS}\")\n",
    "print(f\"Label2ID: {label2id}\")\n",
    "print(f\"ID2Label: {id2label}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Eval Strategy: {EVAL_STRATEGY}\")\n",
    "print(f\"Load Best Model: {LOAD_BEST_MODEL_AT_END}\")\n",
    "print(f\"Metric for Best Model: {METRIC_FOR_BEST_MODEL}\")\n",
    "print(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"LoRA Parameters: r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}, target_modules={LORA_TARGET_MODULES}\")\n",
    "print(\"--- Configuration loaded ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9717fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and preparing FULL RAVDESS data ---\n",
      "Found 1440 audio files from all Actor folders.\n",
      "\n",
      "Label Distribution (Mapped):\n",
      "label\n",
      "neutral      288\n",
      "happy        192\n",
      "sad          192\n",
      "angry        192\n",
      "fearful      192\n",
      "disgust      192\n",
      "surprised    192\n",
      "Name: count, dtype: int64\n",
      "--- Initial DataFrame created ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 3: Load and Prepare RAVDESS Data\n",
    "# Scan the RAVDESS directory, filter audio files based on naming convention, and create an initial DataFrame.\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Loading and preparing FULL RAVDESS data ---\")\n",
    "# =====================================================\n",
    "\n",
    "audio_files = []\n",
    "emotion_labels = []\n",
    "\n",
    "if not os.path.exists(RAVDESS_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"The specified RAVDESS path does not exist: {RAVDESS_DATA_PATH}\\nPlease double-check the path and Cell 2.\")\n",
    "\n",
    "# Use glob to find all wav files recursively within ALL actor folders\n",
    "for file_path in glob.glob(os.path.join(RAVDESS_DATA_PATH, \"Actor_*\", \"*.wav\")):\n",
    "    basename = os.path.basename(file_path)\n",
    "    try:\n",
    "        parts = basename.split('.')[0].split('-')\n",
    "        # Filter based on RAVDESS filename structure\n",
    "        if len(parts) == 7 and parts[0] == TARGET_MODALITY and parts[1] == TARGET_VOCAL_CHANNEL:\n",
    "            emotion_code = parts[2]\n",
    "            if emotion_code in RAVDESS_EMOTION_MAP:\n",
    "                audio_files.append(file_path)\n",
    "                # Use the mapped emotion label\n",
    "                emotion_labels.append(RAVDESS_EMOTION_MAP[emotion_code])\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse filename {basename}: {e}\")\n",
    "\n",
    "\n",
    "if not audio_files:\n",
    "     raise ValueError(f\"No audio files matching the criteria (Modality={TARGET_MODALITY}, VocalChannel={TARGET_VOCAL_CHANNEL}) found in {RAVDESS_DATA_PATH}. Check the path and file naming.\")\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files from all Actor folders.\")\n",
    "\n",
    "# --- Create a Pandas DataFrame ---\n",
    "df = pd.DataFrame({\"audio\": audio_files, \"label\": emotion_labels})\n",
    "\n",
    "# --- Check label distribution (using the mapped labels) ---\n",
    "print(\"\\nLabel Distribution (Mapped):\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# --- Clean up unused variables ---\n",
    "del audio_files\n",
    "del emotion_labels\n",
    "gc.collect()\n",
    "print(\"--- Initial DataFrame created ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ea16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting data into train, validation, and test sets ---\n",
      "Train set size: 1008\n",
      "Validation set size: 216\n",
      "Test set size: 216\n",
      "\n",
      "Raw datasets structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 1008\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 216\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 216\n",
      "    })\n",
      "})\n",
      "--- Data split and DatasetDict created ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 4: Split Data and Create Datasets\n",
    "# Split the DataFrame into train, validation, and test sets, then convert them to Hugging Face `Dataset` objects within a `DatasetDict`.\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Splitting data into train, validation, and test sets ---\")\n",
    "# =====================================================\n",
    "\n",
    "# Use a standard 70% train, 15% validation, 15% test split\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=SEED, stratify=df['label'])\n",
    "# Calculate validation split size relative to the remaining data (train_val_df)\n",
    "val_size_relative = 0.15 / (1.0 - 0.15) # 0.15 / 0.85\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_relative, random_state=SEED, stratify=train_val_df['label'])\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# --- Convert Pandas DataFrames to Hugging Face Datasets ---\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# --- Combine into a DatasetDict ---\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nRaw datasets structure:\")\n",
    "print(raw_datasets)\n",
    "\n",
    "# --- Clean up ---\n",
    "del df\n",
    "del train_val_df\n",
    "del test_df\n",
    "del train_df\n",
    "del val_df\n",
    "del train_dataset\n",
    "del val_dataset\n",
    "del test_dataset\n",
    "gc.collect()\n",
    "print(\"--- Data split and DatasetDict created ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c6f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Feature Extractor and Augmentation ---\n",
      "Wav2Vec2FeatureExtractor loaded.\n",
      "Target sampling rate: 16000\n",
      "Audio augmenter configured.\n",
      "Casting 'audio' column to Audio feature type...\n",
      "Successfully cast 'audio' column for all splits.\n",
      "\n",
      "Example audio feature info after casting:\n",
      "Audio(sampling_rate=16000, mono=True, decode=True, id=None)\n",
      "Preprocessing function defined. Max duration: 4.0s\n",
      "--- Feature extractor, augmentation, and preprocessing function set up ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 5: Feature Extractor, Augmentation, and Preprocessing Function\n",
    "# Load the feature extractor, set up audio augmentation, and define the function to process audio data (including casting, resampling, augmenting, tokenizing, padding/truncating).\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Setting up Feature Extractor and Augmentation ---\")\n",
    "# =====================================================\n",
    "\n",
    "# --- Load Feature Extractor ---\n",
    "# Use the specific class if known, otherwise AutoFeatureExtractor is fine\n",
    "try:\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID, do_normalize=True)\n",
    "    print(\"Wav2Vec2FeatureExtractor loaded.\")\n",
    "except Exception:\n",
    "    print(\"Wav2Vec2FeatureExtractor not found, falling back to AutoFeatureExtractor.\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID, do_normalize=True)\n",
    "\n",
    "TARGET_SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "print(f\"Target sampling rate: {TARGET_SAMPLING_RATE}\")\n",
    "\n",
    "# --- Setup Audio Augmentation ---\n",
    "augmenter = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.010, p=0.3), # Reduced amplitude noise\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.3),\n",
    "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.3, leave_length_unchanged=False), # Time stretch slightly\n",
    "])\n",
    "print(\"Audio augmenter configured.\")\n",
    "\n",
    "# --- Cast the 'audio' column -- Essential Step! ---\n",
    "# This performs resampling upon loading\n",
    "try:\n",
    "    print(\"Casting 'audio' column to Audio feature type...\")\n",
    "    raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "    print(\"Successfully cast 'audio' column for all splits.\")\n",
    "    print(\"\\nExample audio feature info after casting:\")\n",
    "    print(raw_datasets[\"train\"].features[\"audio\"])\n",
    "except Exception as e:\n",
    "    print(f\"\\nError casting audio column: {e}\")\n",
    "    print(\"Ensure 'ffmpeg' is installed and accessible in your environment.\")\n",
    "    print(\"Try: 'conda install ffmpeg' or 'sudo apt update && sudo apt install ffmpeg'\")\n",
    "    raise\n",
    "\n",
    "# --- Define the preprocessing function ---\n",
    "MAX_DURATION_SEC = 4.0 # Adjust if needed based on typical RAVDESS length\n",
    "\n",
    "def preprocess_function(examples, is_train=False):\n",
    "    try:\n",
    "        # Load audio arrays using the datasets' Audio feature (already resampled)\n",
    "        audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "        # Get rate from the first item, should match TARGET_SAMPLING_RATE after casting\n",
    "        sampling_rate = examples[\"audio\"][0][\"sampling_rate\"]\n",
    "\n",
    "        # --- Apply augmentation ONLY to training data ---\n",
    "        if is_train:\n",
    "            processed_audio_arrays = []\n",
    "            for audio_array in audio_arrays:\n",
    "                 # Ensure audio is float32 for audiomentations\n",
    "                 if audio_array.dtype != np.float32:\n",
    "                      audio_array = audio_array.astype(np.float32)\n",
    "                 # Apply augmentation\n",
    "                 augmented_audio = augmenter(samples=audio_array, sample_rate=sampling_rate)\n",
    "                 processed_audio_arrays.append(augmented_audio)\n",
    "            audio_arrays = processed_audio_arrays # Replace with augmented audio\n",
    "        # --- End Augmentation ---\n",
    "\n",
    "        # Apply feature extractor (padding/truncation)\n",
    "        inputs = feature_extractor(\n",
    "            audio_arrays,\n",
    "            sampling_rate=sampling_rate,\n",
    "            max_length=int(sampling_rate * MAX_DURATION_SEC),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\", # Pad to max_length\n",
    "            return_attention_mask=True, # Ensure attention mask is returned\n",
    "            return_tensors=\"np\" # Return numpy arrays initially\n",
    "        )\n",
    "\n",
    "        # Map labels to IDs and ensure correct dtype for PyTorch\n",
    "        label_ids = [label2id[label] for label in examples[\"label\"]]\n",
    "        inputs[\"labels\"] = np.array(label_ids, dtype=np.int64) # Use int64 for CrossEntropyLoss\n",
    "\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing batch: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Return an empty dict or raise error depending on desired behavior\n",
    "        # Returning empty might cause issues later, raising is safer\n",
    "        raise e\n",
    "\n",
    "print(f\"Preprocessing function defined. Max duration: {MAX_DURATION_SEC}s\")\n",
    "print(\"--- Feature extractor, augmentation, and preprocessing function set up ---\")\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2283c3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Preprocessing ---\n",
      "\n",
      "Applying preprocessing function without augmentation for validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 216/216 [00:02<00:00, 75.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing function without augmentation for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 216/216 [00:01<00:00, 195.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing function WITH augmentation for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1008/1008 [00:20<00:00, 49.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_values', 'attention_mask', 'labels'],\n",
      "        num_rows: 1008\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_values', 'attention_mask', 'labels'],\n",
      "        num_rows: 216\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_values', 'attention_mask', 'labels'],\n",
      "        num_rows: 216\n",
      "    })\n",
      "})\n",
      "\n",
      "Example processed train instance keys: dict_keys(['input_values', 'attention_mask', 'labels'])\n",
      "Feature dtypes in processed train dataset:\n",
      "{'input_values': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None)}\n",
      "--- Preprocessing applied ---\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 6: Apply Preprocessing\n",
    "# Use the `.map()` method to apply the `preprocess_function` to the train, validation, and test sets. Augmentation is applied only to the training set.\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Applying Preprocessing ---\")\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\nApplying preprocessing function without augmentation for validation set...\")\n",
    "processed_val_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"is_train\": False}, # Disable augmentation\n",
    "    remove_columns=[\"audio\", \"label\"] # Remove original columns\n",
    ")\n",
    "\n",
    "print(\"\\nApplying preprocessing function without augmentation for test set...\")\n",
    "processed_test_dataset = raw_datasets[\"test\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"is_train\": False}, # Disable augmentation\n",
    "    remove_columns=[\"audio\", \"label\"]\n",
    ")\n",
    "\n",
    "print(\"\\nApplying preprocessing function WITH augmentation for train set...\")\n",
    "# Apply mapping to the train set separately to enable augmentation\n",
    "processed_train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"is_train\": True}, # Enable augmentation\n",
    "    remove_columns=[\"audio\", \"label\"]\n",
    ")\n",
    "\n",
    "\n",
    "# --- Combine into a new Processed DatasetDict ---\n",
    "processed_datasets = DatasetDict({\n",
    "    \"train\": processed_train_dataset,\n",
    "    \"validation\": processed_val_dataset,\n",
    "    \"test\": processed_test_dataset\n",
    "})\n",
    "\n",
    "print(\"\\nProcessed datasets structure:\")\n",
    "print(processed_datasets)\n",
    "print(\"\\nExample processed train instance keys:\", processed_datasets[\"train\"][0].keys())\n",
    "# Check dtypes after processing\n",
    "print(\"Feature dtypes in processed train dataset:\")\n",
    "print(processed_datasets[\"train\"].features)\n",
    "\n",
    "\n",
    "# --- Clean up ---\n",
    "del processed_train_dataset\n",
    "del processed_val_dataset\n",
    "del processed_test_dataset\n",
    "del raw_datasets # We don't need the raw audio data loaded anymore\n",
    "gc.collect()\n",
    "print(\"--- Preprocessing applied ---\")\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00263523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading pre-trained model for fine-tuning (PEFT/LoRA) ---\n",
      "Loading base model: superb/wav2vec2-base-superb-er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded and set to float32.\n",
      "Original base model total parameters: 94,570,388\n",
      "Identified classifier modules to keep trainable: ['classifier', 'projector']\n",
      "\n",
      "--- Setting up LoRA Configuration ---\n",
      "LoRA Config: LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=32, target_modules={'q_proj', 'k_proj', 'out_proj', 'v_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'projector'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "Applying monkey-patch to model.enable_input_require_grads...\n",
      "\n",
      "Applying get_peft_model...\n",
      "Skipping enable_input_require_grads via monkey-patch for Wav2Vec2.\n",
      "Base model wrapped with LoRA adapters.\n",
      "\n",
      "--- PEFT Model Parameters ---\n",
      "trainable params: 2,557,959 || all params: 97,128,347 || trainable%: 2.6336\n",
      "\n",
      "PEFT Model moved to cuda\n",
      "Attempting to restore original model.enable_input_require_grads...\n",
      "Original method potentially restored.\n",
      "\n",
      "--- Model loading and PEFT setup complete ---\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 7: Load Base Model and Apply PEFT (LoRA)\n",
    "# Load the pre-trained base model, calculate its parameters, set up LoRA, apply PEFT, and print the trainable parameters.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Loading pre-trained model for fine-tuning (PEFT/LoRA) ---\")\n",
    "# =====================================================\n",
    "# Clear some memory before loading model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "# --- Load the base model ---\n",
    "print(f\"Loading base model: {MODEL_ID}\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=NUM_LABELS,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    # low_cpu_mem_usage=True, # Optional\n",
    ")\n",
    "model = model.float() # Ensure float32\n",
    "print(\"Base model loaded and set to float32.\")\n",
    "\n",
    "# --- Calculate and Print Original Parameters ---\n",
    "total_params_original = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Original base model total parameters: {total_params_original:,}\") # Added comma formatting\n",
    "\n",
    "# --- Find classifier module names ---\n",
    "classifier_modules = []\n",
    "if hasattr(model, 'classifier'): classifier_modules.append(\"classifier\")\n",
    "if hasattr(model, 'projector'): classifier_modules.append(\"projector\")\n",
    "if hasattr(model, 'output_projection'): classifier_modules.append(\"output_projection\")\n",
    "if not classifier_modules:\n",
    "    print(\"WARNING: Could not find default classifier names. Defaulting to 'classifier'.\")\n",
    "    classifier_modules = [\"classifier\"]\n",
    "print(f\"Identified classifier modules to keep trainable: {classifier_modules}\")\n",
    "\n",
    "# --- Setup LoRA Configuration ---\n",
    "print(\"\\n--- Setting up LoRA Configuration ---\")\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    modules_to_save=classifier_modules,\n",
    ")\n",
    "print(f\"LoRA Config: {lora_config}\")\n",
    "\n",
    "# **** MONKEY-PATCH SECTION ****\n",
    "# (Keep this section as it was)\n",
    "def dummy_enable_input_require_grads(self):\n",
    "    print(\"Skipping enable_input_require_grads via monkey-patch for Wav2Vec2.\")\n",
    "    pass\n",
    "original_enable_grads_func = None\n",
    "base_model_ref = model\n",
    "if hasattr(base_model_ref, 'enable_input_require_grads'):\n",
    "    original_enable_grads_func = base_model_ref.enable_input_require_grads\n",
    "    print(\"Applying monkey-patch to model.enable_input_require_grads...\")\n",
    "    base_model_ref.enable_input_require_grads = types.MethodType(dummy_enable_input_require_grads, base_model_ref)\n",
    "else:\n",
    "    print(\"Model does not have 'enable_input_require_grads' method to patch.\")\n",
    "# ****************************\n",
    "\n",
    "# --- Apply LoRA to the base model ---\n",
    "print(\"\\nApplying get_peft_model...\")\n",
    "peft_model = None\n",
    "try:\n",
    "    # Use the potentially patched 'model' (which is base_model_ref)\n",
    "    peft_model = get_peft_model(base_model_ref, lora_config)\n",
    "    print(\"Base model wrapped with LoRA adapters.\")\n",
    "\n",
    "    # --- Print Trainable Parameters (Shows Reduction) ---\n",
    "    print(\"\\n--- PEFT Model Parameters ---\")\n",
    "    peft_model.print_trainable_parameters() # THIS IS THE KEY PEFT FUNCTION\n",
    "\n",
    "    # --- Move PEFT model to GPU if available ---\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\"); peft_model.to(device)\n",
    "        print(f\"\\nPEFT Model moved to {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\"); print(f\"\\nCUDA not available. PEFT Model remains on {device}\")\n",
    "\n",
    "    # Reassign the main 'model' variable to the PEFT model for the Trainer\n",
    "    model = peft_model\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error during get_peft_model or subsequent steps: {e} ---\")\n",
    "    traceback.print_exc()\n",
    "    print(\"PEFT model creation failed. 'model' variable holds the original base model.\")\n",
    "    # Ensure model variable still refers to the original if PEFT failed\n",
    "    model = base_model_ref\n",
    "\n",
    "finally:\n",
    "    # (Keep the finally block for restoring the monkey patch as it was)\n",
    "    if original_enable_grads_func is not None:\n",
    "        target_for_restore = None\n",
    "        # Check the type of the 'model' variable *after* the try block\n",
    "        current_model_ref = model\n",
    "        if isinstance(current_model_ref, PeftModel):\n",
    "             target_for_restore = current_model_ref.get_base_model()\n",
    "        elif 'base_model_ref' in locals():\n",
    "             target_for_restore = base_model_ref\n",
    "\n",
    "        if target_for_restore and hasattr(target_for_restore, 'enable_input_require_grads'):\n",
    "             print(\"Attempting to restore original model.enable_input_require_grads...\")\n",
    "             target_for_restore.enable_input_require_grads = original_enable_grads_func\n",
    "             print(\"Original method potentially restored.\")\n",
    "        else:\n",
    "             print(\"Could not restore original enable_input_require_grads.\")\n",
    "\n",
    "print(\"\\n--- Model loading and PEFT setup complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d46d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # ---\n",
    "# # ## Cell 8: Define Evaluation Metrics\n",
    "# # Define the `compute_metrics` function. Using REAL metrics now.\n",
    "# # ---\n",
    "\n",
    "# # %%\n",
    "# # =====================================================\n",
    "# print(\"\\n--- Defining Evaluation Metrics ---\")\n",
    "# # =====================================================\n",
    "\n",
    "# # --- APPROACH 1: Dummy compute_metrics (Commented Out) ---\n",
    "# # def compute_metrics(eval_pred):\n",
    "# #     \"\"\"\n",
    "# #     Dummy compute_metrics function that returns fixed values.\n",
    "# #     Used for debugging the Trainer's evaluation flow.\n",
    "# #     \"\"\"\n",
    "# #     print(\"\\n--- Inside DUMMY compute_metrics ---\")\n",
    "# #     dummy_acc = 0.5; dummy_f1 = 0.5\n",
    "# #     print(f\"  Returning dummy metrics: {{'accuracy': {dummy_acc}, 'f1': {dummy_f1}}}\")\n",
    "# #     return {\"accuracy\": dummy_acc, \"f1\": dummy_f1}\n",
    "# # print(\"DUMMY compute_metrics function defined.\")\n",
    "\n",
    "\n",
    "# # --- REAL COMPUTE_METRICS FUNCTION (ACTIVE) ---\n",
    "# accuracy_metric = evaluate.load(\"accuracy\")\n",
    "# f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     \"\"\"Computes accuracy and F1-score from model predictions.\"\"\"\n",
    "#     print(\"\\n--- Inside REAL compute_metrics ---\")\n",
    "#     logits, labels = eval_pred # Trainer passes numpy arrays here normally\n",
    "#     print(f\"  Initial Logits type: {type(logits)}, Labels type: {type(labels)}\")\n",
    "#     # Logits might sometimes be tuples from prediction_step override, labels should be ndarray\n",
    "#     if isinstance(logits, tuple): # Should not happen with our revised prediction_step, but safe check\n",
    "#         print(\"  Logits is a tuple, taking first element.\")\n",
    "#         logits = logits[0]\n",
    "#         print(f\"  New Logits type: {type(logits)}\")\n",
    "\n",
    "#     # --- Robust Logits Conversion ---\n",
    "#     try:\n",
    "#         if not isinstance(logits, np.ndarray):\n",
    "#             print(f\"  Logits is not numpy array ({type(logits)}). Attempting conversion.\")\n",
    "#             if isinstance(logits, torch.Tensor):\n",
    "#                  print(\"  Attempting conversion from torch.Tensor\")\n",
    "#                  logits = logits.detach().cpu().numpy()\n",
    "#                  print(f\"  Logits converted from tensor. Shape: {logits.shape}\")\n",
    "#             elif isinstance(logits, (list, tuple)) and len(logits) > 0 and all(isinstance(i, np.ndarray) for i in logits):\n",
    "#                  print(\"  Attempting concatenation from list/tuple of arrays.\")\n",
    "#                  logits = np.concatenate(logits, axis=0)\n",
    "#                  print(f\"  Logits concatenated. Shape: {logits.shape}\")\n",
    "#             else:\n",
    "#                  print(f\"  ERROR: Unsupported logits type ({type(logits)}) for conversion.\")\n",
    "#                  return {} # Return empty dict on failure\n",
    "#         else:\n",
    "#              print(f\"  Logits already numpy array. Shape: {logits.shape}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ERROR during logits conversion: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return {}\n",
    "\n",
    "#     # --- Robust Labels Conversion ---\n",
    "#     try:\n",
    "#         if not isinstance(labels, np.ndarray):\n",
    "#             print(f\"  Labels is not numpy array ({type(labels)}). Attempting conversion.\")\n",
    "#             if isinstance(labels, torch.Tensor):\n",
    "#                 print(\"  Attempting conversion from torch.Tensor\")\n",
    "#                 labels = labels.detach().cpu().numpy()\n",
    "#                 print(f\"  Labels converted from tensor. Shape: {labels.shape}\")\n",
    "#             elif isinstance(labels, (list, tuple)) and len(labels) > 0 and all(isinstance(i, np.ndarray) for i in labels):\n",
    "#                  print(\"  Attempting concatenation from list/tuple of arrays.\")\n",
    "#                  labels = np.concatenate(labels, axis=0)\n",
    "#                  print(f\"  Labels concatenated. Shape: {labels.shape}\")\n",
    "#             elif isinstance(labels, (list, tuple)):\n",
    "#                 print(\"  Attempting conversion from list/tuple of numbers.\")\n",
    "#                 labels = np.array(labels)\n",
    "#                 print(f\"  Labels converted from list. Shape: {labels.shape}\")\n",
    "#             else:\n",
    "#                  print(f\"  ERROR: Unsupported labels type ({type(labels)}) for conversion.\")\n",
    "#                  return {}\n",
    "#         else:\n",
    "#             print(f\"  Labels already numpy array. Shape: {labels.shape}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ERROR during labels conversion: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return {}\n",
    "\n",
    "#     # --- Argmax and Shape Check ---\n",
    "#     try:\n",
    "#         # Ensure labels are integer type for comparison\n",
    "#         labels = labels.astype(np.int64)\n",
    "\n",
    "#         # Check if logits has at least 2 dimensions for argmax\n",
    "#         if logits.ndim < 2:\n",
    "#              print(f\"  ERROR: Logits has invalid shape {logits.shape} for argmax.\")\n",
    "#              return {}\n",
    "\n",
    "#         predictions = np.argmax(logits, axis=-1)\n",
    "#         print(f\"  Predictions calculated. Shape: {predictions.shape}, dtype: {predictions.dtype}\")\n",
    "#         print(f\"  Labels shape: {labels.shape}, dtype: {labels.dtype}\") # Verify label shape/dtype\n",
    "\n",
    "#         if predictions.shape != labels.shape:\n",
    "#             print(f\"  ERROR: Shape mismatch! Preds: {predictions.shape}, Labels: {labels.shape}\")\n",
    "#             # Try flatten only if total sizes match\n",
    "#             if predictions.size == labels.size:\n",
    "#                  print(\"  Attempting flatten...\")\n",
    "#                  predictions = predictions.flatten()\n",
    "#                  labels = labels.flatten()\n",
    "#                  print(f\"  New shapes after flatten: Preds: {predictions.shape}, Labels: {labels.shape}\")\n",
    "#                  if predictions.shape != labels.shape:\n",
    "#                       print(\"  ERROR: Shape mismatch persists after flatten.\")\n",
    "#                       return {}\n",
    "#             else:\n",
    "#                  print(\"  Cannot flatten, sizes do not match.\")\n",
    "#                  return {}\n",
    "#         else:\n",
    "#             print(\"  Shapes match.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ERROR during argmax or shape check: {e}\")\n",
    "#         print(f\"  Logits shape was: {logits.shape}\")\n",
    "#         traceback.print_exc()\n",
    "#         return {}\n",
    "\n",
    "#     # --- Calculate Metrics ---\n",
    "#     try:\n",
    "#         print(\"  Calculating accuracy...\")\n",
    "#         acc = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "#         print(f\"  Accuracy: {acc}\")\n",
    "\n",
    "#         print(\"  Calculating F1 score (weighted)...\")\n",
    "#         f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=0)[\"f1\"]\n",
    "#         print(f\"  F1 Score: {f1}\")\n",
    "\n",
    "#         # Keys here ('accuracy', 'f1') must match METRIC_FOR_BEST_MODEL (without 'eval_')\n",
    "#         result = {\"accuracy\": acc, \"f1\": f1}\n",
    "#         print(f\"  Successfully computed REAL metrics. Returning: {result}\") # Confirmation print\n",
    "#         return result\n",
    "#     except Exception as e:\n",
    "#         print(f\"  ERROR during metric calculation: {e}\")\n",
    "#         traceback.print_exc()\n",
    "#         return {}\n",
    "\n",
    "# # Fix the final print statement\n",
    "# print(\"REAL compute_metrics function defined.\")\n",
    "# # --- End of Real compute_metrics block ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71e4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Training Arguments (Evaluation DISABLED) ---\n",
      "CUDA available: True\n",
      "\n",
      "Training arguments set. Output Dir: ./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\n",
      "Evaluation Strategy: no\n",
      "Load best model at end: False\n",
      "FP16 Enabled: True\n",
      "Device: cuda:0, n_gpu: 1\n",
      "--- Training Arguments set for NO EVALUATION ---\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 9: Setup Training Arguments (Evaluation Disabled)\n",
    "# Configure `TrainingArguments` with evaluation turned off.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Setting up Training Arguments (Evaluation DISABLED) ---\")\n",
    "# =====================================================\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {use_cuda}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=FINETUNED_MODEL_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    # per_device_eval_batch_size=BATCH_SIZE * 2, # Not needed\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "\n",
    "    # --- Evaluation Settings (Approach 2 - Disabled) ---\n",
    "    eval_strategy=\"no\",              # <<< DISABLE evaluation strategy\n",
    "    load_best_model_at_end=False,    # <<< DISABLE loading best model\n",
    "    # metric_for_best_model=\"eval_f1\", # <<< COMMENTED OUT / REMOVED\n",
    "    # greater_is_better=True,        # <<< COMMENTED OUT / REMOVED\n",
    "\n",
    "    # --- Other Settings ---\n",
    "    num_train_epochs=NUM_EPOCHS,     # Use the full number of epochs\n",
    "    save_strategy=\"epoch\",           # Still save checkpoints each epoch\n",
    "    fp16=use_cuda,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    push_to_hub=False,\n",
    "    seed=SEED,\n",
    "    report_to=\"tensorboard\",         # Still log training loss to tensorboard\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    max_grad_norm=1.0,\n",
    "    no_cuda=not use_cuda,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining arguments set. Output Dir: {training_args.output_dir}\")\n",
    "print(f\"Evaluation Strategy: {training_args.eval_strategy}\") # Should print 'no'\n",
    "print(f\"Load best model at end: {training_args.load_best_model_at_end}\") # Should print False\n",
    "print(f\"FP16 Enabled: {training_args.fp16}\")\n",
    "print(f\"Device: {training_args.device}, n_gpu: {training_args.n_gpu}\")\n",
    "print(\"--- Training Arguments set for NO EVALUATION ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f959ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining Data Collator ---\n",
      "Data Collator defined.\n",
      "\n",
      "--- Defining DebugTrainer ---\n",
      "DebugTrainer class defined (prediction_step modified for inspection, but won't be called during train).\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 10: Define Data Collator and DebugTrainer\n",
    "# Set up the data collator and the custom `DebugTrainer` class.\n",
    "# ---\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Defining Data Collator ---\")\n",
    "# =====================================================\n",
    "data_collator = DataCollatorWithPadding(tokenizer=feature_extractor, padding=True)\n",
    "print(\"Data Collator defined.\")\n",
    "\n",
    "# =====================================================\n",
    "print(\"\\n--- Defining DebugTrainer ---\")\n",
    "# =====================================================\n",
    "class DebugTrainer(Trainer):\n",
    "    # Keep training_step as is\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        if self.state.global_step <= 1 or self.state.global_step % (self.args.logging_steps * 20) == 0 :\n",
    "            try:\n",
    "                input_device = inputs['input_values'].device; label_device = inputs['labels'].device if 'labels' in inputs else 'N/A'\n",
    "                expected_device = self.args.device; model_param_device = next(model.parameters()).device if len(list(model.parameters())) > 0 else \"N/A\"\n",
    "                print(f\"\\n--- DebugTrainer Step {self.state.global_step} ---\")\n",
    "                print(f\"  Input device: {input_device}, Label device: {label_device}, Expected: {expected_device}, Model param: {model_param_device}\")\n",
    "                if input_device != expected_device or (label_device != 'N/A' and label_device != expected_device): print(f\"  !! WARNING: Device mismatch detected!\")\n",
    "            except Exception as e: print(f\"  DebugTrainer device check failed: {e}\")\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if labels.dtype != torch.long: inputs[\"labels\"] = labels.long()\n",
    "        else: print(f\"ERROR: 'labels' key missing from training inputs at step {self.state.global_step}!\")\n",
    "        try: return super().training_step(model, inputs, num_items_in_batch)\n",
    "        except Exception as e: print(f\"Error in super().training_step at step {self.state.global_step}: {e}\"); traceback.print_exc(); raise e\n",
    "\n",
    "    # Keep prediction_step with logging (won't be called during train)\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        print(f\"\\n--- DebugTrainer prediction_step (prediction_loss_only={prediction_loss_only}) ---\")\n",
    "        has_labels = \"labels\" in inputs; input_labels = None\n",
    "        print(f\"  Input keys: {list(inputs.keys())}\"); print(f\"  Has Labels: {has_labels}\")\n",
    "        if has_labels:\n",
    "             input_labels = inputs[\"labels\"]\n",
    "             print(f\"  Initial label dtype: {input_labels.dtype}, shape: {input_labels.shape}\")\n",
    "             if input_labels.dtype != torch.long: inputs[\"labels\"] = input_labels.long(); input_labels = inputs[\"labels\"]\n",
    "        try:\n",
    "            loss, logits_raw_output, labels_out = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "            print(f\"  Raw logits_raw_output type: {type(logits_raw_output)}\")\n",
    "            if isinstance(logits_raw_output, tuple):\n",
    "                 print(f\"  Raw logits_raw_output is a tuple of length: {len(logits_raw_output)}\")\n",
    "                 for i, item in enumerate(logits_raw_output):\n",
    "                     print(f\"    Item {i} type: {type(item)}\")\n",
    "                     if hasattr(item, 'shape'): print(f\"    Item {i} shape: {item.shape}\")\n",
    "                     if hasattr(item, 'dtype'): print(f\"    Item {i} dtype: {item.dtype}\")\n",
    "            elif hasattr(logits_raw_output, 'shape'): print(f\"  Raw logits_raw_output shape: {logits_raw_output.shape}\")\n",
    "            logits = None\n",
    "            if isinstance(logits_raw_output, tuple):\n",
    "                print(\"  Attempting extraction from tuple...\")\n",
    "                # --- CORRECTED EXTRACTION ---\n",
    "                if len(logits_raw_output) > 1 and isinstance(logits_raw_output[1], torch.Tensor):\n",
    "                     print(\"  Assuming logits are the SECOND element (index 1) of the tuple.\")\n",
    "                     logits = logits_raw_output[1] # <<< GRAB ITEM 1\n",
    "                # Fallback 1: Check first element\n",
    "                elif len(logits_raw_output) > 0 and isinstance(logits_raw_output[0], torch.Tensor):\n",
    "                     print(\"  Falling back: Assuming logits are the FIRST element of the tuple.\")\n",
    "                     logits = logits_raw_output[0]\n",
    "                # Fallback 2: Check attribute\n",
    "                elif len(logits_raw_output) > 0 and hasattr(logits_raw_output[0], 'logits'):\n",
    "                     print(\"  Falling back: Assuming logits are in '.logits' attribute of the first element.\")\n",
    "                     logits = logits_raw_output[0].logits\n",
    "                else:\n",
    "                     print(\"  ERROR: Cannot determine logits structure in tuple.\")\n",
    "                     logits = torch.empty(0)\n",
    "            elif isinstance(logits_raw_output, torch.Tensor):\n",
    "                 print(\"  Logits output is already a tensor.\")\n",
    "                 logits = logits_raw_output\n",
    "            else: print(f\"  ERROR: Unexpected type for logits_raw_output: {type(logits_raw_output)}\"); logits = torch.empty(0)\n",
    "            if labels_out is None and input_labels is not None: print(\"  Using input labels for labels_out.\"); labels_out = input_labels\n",
    "            print(f\"  prediction_step processed.\"); print(f\"  Returned loss type: {type(loss)}\")\n",
    "            print(f\"  Processed logits type: {type(logits)}, shape: {logits.shape if hasattr(logits, 'shape') else 'N/A'}\")\n",
    "            print(f\"  Processed labels_out type: {type(labels_out)}, shape: {labels_out.shape if hasattr(labels_out, 'shape') else 'N/A'}\")\n",
    "            if isinstance(logits, torch.Tensor): logits = logits.detach()\n",
    "            if isinstance(labels_out, torch.Tensor): labels_out = labels_out.detach()\n",
    "            if not (hasattr(logits, 'ndim') and logits.ndim == 2): print(f\"  WARNING: Processed logits shape {logits.shape if hasattr(logits,'shape') else 'N/A'} is NOT 2D.\")\n",
    "            if labels_out is not None and not (hasattr(labels_out, 'ndim') and labels_out.ndim == 1): print(f\"  WARNING: Processed labels_out shape {labels_out.shape if hasattr(labels_out,'shape') else 'N/A'} is NOT 1D.\")\n",
    "            return (loss, logits, labels_out)\n",
    "        except Exception as e: print(f\"Error in super().prediction_step or processing: {e}\"); traceback.print_exc(); raise e\n",
    "print(\"DebugTrainer class defined (prediction_step modified for inspection, but won't be called during train).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c2adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing Trainer (Evaluation DISABLED) ---\n",
      "Trainer initialized for NO EVALUATION.\n",
      "Compute metrics function assigned: None\n",
      "Evaluation dataset assigned: No\n",
      "Callbacks assigned: [<transformers.trainer_callback.DefaultFlowCallback object at 0x0000023CE95BB850>, <transformers.integrations.integration_utils.TensorBoardCallback object at 0x0000023CE95BB4C0>, <transformers.utils.notebook.NotebookProgressCallback object at 0x0000023CE95B9180>]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 11: Initialize Trainer (Evaluation Disabled)\n",
    "# Instantiate `DebugTrainer` without evaluation components.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Initializing Trainer (Evaluation DISABLED) ---\")\n",
    "# =====================================================\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "# Callbacks related to evaluation (like EarlyStopping) are removed\n",
    "# compute_metrics and eval_dataset are removed\n",
    "trainer = DebugTrainer(\n",
    "    model=model,                        # Pass the PEFT model\n",
    "    args=training_args,                 # Use args configured for NO EVALUATION\n",
    "\n",
    "    # --- Evaluation Components DISABLED ---\n",
    "    # eval_dataset=processed_datasets[\"validation\"], # <<< COMMENTED OUT\n",
    "    # compute_metrics=compute_metrics,             # <<< COMMENTED OUT (and Cell 8 commented)\n",
    "    # callbacks=[early_stopping_callback],         # <<< COMMENTED OUT\n",
    "\n",
    "    # --- Other Components ---\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer initialized for NO EVALUATION.\")\n",
    "print(f\"Compute metrics function assigned: {'None'}\") # Expect None\n",
    "print(f\"Evaluation dataset assigned: {'No'}\") # Expect No\n",
    "print(f\"Callbacks assigned: {trainer.callback_handler.callbacks}\") # Should be default callbacks only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b6123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pre-Training Checks (Evaluation Disabled) ---\n",
      "Trainer device: cuda:0\n",
      "Model device: cuda:0\n",
      "Evaluation disabled, skipping related checks.\n",
      "FP16: True\n",
      "\n",
      "--- Skipping Manual Evaluation Call (Evaluation Disabled) ---\n",
      "--- Pre-training checks finished ---\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 12: Pre-Training Checks (Evaluation Disabled)\n",
    "# Verify settings. Manual evaluation is skipped.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Pre-Training Checks (Evaluation Disabled) ---\")\n",
    "# =====================================================\n",
    "try:\n",
    "    print(f\"Trainer device: {trainer.args.device}\")\n",
    "    if len(list(trainer.model.parameters())) > 0:\n",
    "        model_param_device = next(trainer.model.parameters()).device\n",
    "        print(f\"Model device: {model_param_device}\")\n",
    "    else: print(\"Model has no parameters.\")\n",
    "    # No eval dataset or compute metrics to check\n",
    "    print(\"Evaluation disabled, skipping related checks.\")\n",
    "    print(f\"FP16: {trainer.args.fp16}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during pre-flight checks: {e}\"); traceback.print_exc()\n",
    "\n",
    "# --- Manual evaluation skipped ---\n",
    "print(\"\\n--- Skipping Manual Evaluation Call (Evaluation Disabled) ---\")\n",
    "print(\"--- Pre-training checks finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482027c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Fine-Tuning (PEFT/LoRA Run - Evaluation DISABLED) ---\n",
      "CUDA cache cleared.\n",
      "\n",
      "--- Trainer object details before train() ---\n",
      "  Compute metrics: None\n",
      "  Eval dataset: None\n",
      "\n",
      "--- Starting Training (Epochs: 15) ---\n",
      "\n",
      "--- DebugTrainer Step 0 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 0 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 0 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 0 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [945/945 27:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.937800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DebugTrainer Step 1 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 1 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 1 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- DebugTrainer Step 1 ---\n",
      "  Input device: cuda:0, Label device: cuda:0, Expected: cuda:0, Model param: cuda:0\n",
      "\n",
      "--- Training Finished ---\n",
      "***** train metrics *****\n",
      "  epoch                    =        15.0\n",
      "  total_flos               = 525205352GF\n",
      "  train_loss               =      1.3549\n",
      "  train_runtime            =  0:27:07.07\n",
      "  train_samples            =        1008\n",
      "  train_samples_per_second =       9.293\n",
      "  train_steps_per_second   =       0.581\n",
      "\n",
      "Saving final model state (from last epoch)...\n",
      "\n",
      "Saving final epoch LoRA adapter separately to: ./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\final_epoch_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter 'final_epoch_adapter' saved.\n",
      "\n",
      "Cleaning up training objects...\n",
      "Deleting model...\n",
      "Deleting trainer...\n",
      "Deleting datasets...\n",
      "CUDA cache cleared.\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 13: Start Fine-Tuning (Evaluation Disabled)\n",
    "# Run `trainer.train()`. It should now complete without evaluation errors.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Starting Fine-Tuning (PEFT/LoRA Run - Evaluation DISABLED) ---\")\n",
    "# =====================================================\n",
    "gc.collect(); torch.cuda.empty_cache(); print(\"CUDA cache cleared.\")\n",
    "train_result = None\n",
    "try:\n",
    "    print(\"\\n--- Trainer object details before train() ---\")\n",
    "    print(f\"  Compute metrics: None\") # Expect None\n",
    "    print(f\"  Eval dataset: None\") # Expect None\n",
    "\n",
    "    print(f\"\\n--- Starting Training (Epochs: {NUM_EPOCHS}) ---\") # Use full NUM_EPOCHS\n",
    "    # --- Start Training ---\n",
    "    train_result = trainer.train() # Should run without evaluation steps/errors\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "    # Log & Save final metrics from the training run (will only contain train loss etc)\n",
    "    metrics = train_result.metrics\n",
    "    # Ensure processed_datasets exists if needed here - it might have been deleted by error handling previously\n",
    "    if 'processed_datasets' in locals() and processed_datasets and \"train\" in processed_datasets:\n",
    "        metrics[\"train_samples\"] = len(processed_datasets[\"train\"])\n",
    "    else:\n",
    "         # Attempt to get length from trainer if dataset was deleted\n",
    "         try:\n",
    "             metrics[\"train_samples\"] = len(trainer.train_dataset)\n",
    "         except:\n",
    "             metrics[\"train_samples\"] = 'N/A' # Fallback\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "    # Save the final model state (from the last epoch)\n",
    "    print(\"\\nSaving final model state (from last epoch)...\")\n",
    "    trainer.save_model() # Saves model from last epoch\n",
    "\n",
    "    # Save the final PEFT adapter separately\n",
    "    adapter_folder_name = \"final_epoch_adapter\" # Name reflects lack of validation\n",
    "    final_adapter_path = os.path.join(FINETUNED_MODEL_OUTPUT_DIR, adapter_folder_name)\n",
    "    print(f\"\\nSaving final epoch LoRA adapter separately to: {final_adapter_path}\")\n",
    "    model_to_save = trainer.model\n",
    "    if hasattr(model_to_save, 'save_pretrained'):\n",
    "        model_to_save.save_pretrained(final_adapter_path)\n",
    "        if trainer.tokenizer: trainer.tokenizer.save_pretrained(final_adapter_path)\n",
    "        print(f\"Adapter '{adapter_folder_name}' saved.\")\n",
    "    elif hasattr(model_to_save, 'module') and hasattr(model_to_save.module, 'save_pretrained'):\n",
    "        model_to_save.module.save_pretrained(final_adapter_path)\n",
    "        if trainer.tokenizer: trainer.tokenizer.save_pretrained(final_adapter_path)\n",
    "        print(f\"Adapter '{adapter_folder_name}' saved (from wrapped).\")\n",
    "    else: print(f\"WARNING: Could not save adapter '{adapter_folder_name}' separately.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Error during training: {e} ---\"); traceback.print_exc()\n",
    "finally:\n",
    "    # Cleanup remains the same\n",
    "    print(\"\\nCleaning up training objects...\")\n",
    "    # Use try-except for deletion just in case variables don't exist after error\n",
    "    try:\n",
    "        if 'model' in locals() or 'model' in globals(): print(\"Deleting model...\"); del model\n",
    "    except NameError: pass\n",
    "    try:\n",
    "        if 'trainer' in locals() or 'trainer' in globals(): print(\"Deleting trainer...\"); del trainer\n",
    "    except NameError: pass\n",
    "    try:\n",
    "        if 'processed_datasets' in locals() or 'processed_datasets' in globals(): print(\"Deleting datasets...\"); del processed_datasets\n",
    "    except NameError: pass\n",
    "    gc.collect(); torch.cuda.empty_cache(); print(\"CUDA cache cleared.\"); print(\"Cleanup finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51523f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Adapter path to load: ./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\final_epoch_adapter\n",
      "\n",
      "Loading base model: superb/wav2vec2-base-superb-er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded for evaluation.\n",
      "Applying monkey-patch to base_model before loading adapter...\n",
      "Monkey-patch applied for evaluation loading.\n",
      "Loading adapter from: ./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\final_epoch_adapter\n",
      "Skipping enable_input_require_grads via monkey-patch during eval loading.\n",
      "PEFT adapter loaded onto base model.\n",
      "Restoring original enable_input_require_grads on base_model...\n",
      "Evaluation model moved to cuda\n",
      "Explicitly setting evaluation model to FP32 (.float())...\n",
      "Applying type-fixing wrapper to prevent tensor type errors...\n",
      "Applied type-fixing forward wrapper to model.\n",
      "Loading feature extractor saved with adapter...\n",
      "Checking/Reloading Test Dataset for evaluation...\n",
      "Re-creating test dataset split...\n",
      "Re-splitting data...\n",
      "Applying preprocessing to test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 216/216 [00:01<00:00, 148.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset re-created and processed.\n",
      "\n",
      "Defining REAL compute_metrics for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL compute_metrics_for_eval function defined.\n",
      "\n",
      "Creating type-fixing data collator...\n",
      "Type-fixing data collator created.\n",
      "\n",
      "Initializing standard evaluation Trainer...\n",
      "Evaluation FP16 set to: False\n",
      "Standard evaluation Trainer initialized.\n",
      "\n",
      "Running evaluation on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Evaluation Results ---\n",
      "{'eval_model_preparation_time': 0.0223, 'eval_runtime': 36.3727, 'eval_samples_per_second': 5.939, 'eval_steps_per_second': 0.742}\n",
      "Test metrics saved to ././wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora/eval_test_final_epoch\\eval_results.json\n",
      "\n",
      "Cleaning up evaluation objects...\n",
      "Evaluation cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# =====================================================\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "# --- Determine which adapter to load ---\n",
    "adapter_folder_name = \"final_epoch_adapter\" # Correct for disabled eval during train\n",
    "adapter_path = os.path.join(FINETUNED_MODEL_OUTPUT_DIR, adapter_folder_name)\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    print(f\"Adapter path to load: {adapter_path}\")\n",
    "\n",
    "    # --- Load the base model again ---\n",
    "    print(f\"\\nLoading base model: {MODEL_ID}\")\n",
    "    config = AutoConfig.from_pretrained(MODEL_ID, num_labels=NUM_LABELS, label2id=label2id, id2label=id2label)\n",
    "    base_model = AutoModelForAudioClassification.from_pretrained(MODEL_ID, config=config, ignore_mismatched_sizes=True)\n",
    "    base_model = base_model.float()\n",
    "    print(\"Base model loaded for evaluation.\")\n",
    "\n",
    "    # --- Apply Monkey-Patch ---\n",
    "    print(\"Applying monkey-patch to base_model before loading adapter...\")\n",
    "    original_enable_grads_func_eval = None\n",
    "    if hasattr(base_model, 'enable_input_require_grads'):\n",
    "        original_enable_grads_func_eval = base_model.enable_input_require_grads\n",
    "        def dummy_enable_input_require_grads_eval(self): print(\"Skipping enable_input_require_grads via monkey-patch during eval loading.\"); pass\n",
    "        base_model.enable_input_require_grads = types.MethodType(dummy_enable_input_require_grads_eval, base_model)\n",
    "        print(\"Monkey-patch applied for evaluation loading.\")\n",
    "    else: print(\"Base model does not have 'enable_input_require_grads' method to patch.\")\n",
    "\n",
    "    # --- Load the PEFT adapter ---\n",
    "    print(f\"Loading adapter from: {adapter_path}\")\n",
    "    eval_model = None\n",
    "    try:\n",
    "        eval_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(\"PEFT adapter loaded onto base model.\")\n",
    "        if original_enable_grads_func_eval is not None and hasattr(base_model, 'enable_input_require_grads'):\n",
    "             print(\"Restoring original enable_input_require_grads on base_model...\")\n",
    "             base_model.enable_input_require_grads = original_enable_grads_func_eval\n",
    "\n",
    "        # --- Move to device ---\n",
    "        target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        eval_model.to(target_device)\n",
    "        print(f\"Evaluation model moved to {target_device}\")\n",
    "\n",
    "        # --- FORCE MODEL TO FP32 ---\n",
    "        print(\"Explicitly setting evaluation model to FP32 (.float())...\")\n",
    "        eval_model = eval_model.float()\n",
    "        # --- END FP32 FORCE ---\n",
    "        \n",
    "        # --- Type-fixing wrapper to solve the error ---\n",
    "        print(\"Applying type-fixing wrapper to prevent tensor type errors...\")\n",
    "        original_forward = eval_model.forward\n",
    "        \n",
    "        def type_safe_forward(*args, **kwargs):\n",
    "            # Ensure labels are Long (int64)\n",
    "            if 'labels' in kwargs and kwargs['labels'] is not None:\n",
    "                kwargs['labels'] = kwargs['labels'].long().to(kwargs['labels'].device)\n",
    "                \n",
    "            # Call original forward\n",
    "            outputs = original_forward(*args, **kwargs)\n",
    "            \n",
    "            # Ensure logits are float32\n",
    "            if hasattr(outputs, 'logits') and outputs.logits is not None:\n",
    "                outputs.logits = outputs.logits.to(torch.float32)\n",
    "                \n",
    "            return outputs\n",
    "            \n",
    "        eval_model.forward = type_safe_forward\n",
    "        print(\"Applied type-fixing forward wrapper to model.\")\n",
    "        # --- End type-fixing wrapper ---\n",
    "\n",
    "        eval_model.eval() # Set model to evaluation mode\n",
    "\n",
    "        # --- Reload Feature Extractor ---\n",
    "        print(\"Loading feature extractor saved with adapter...\")\n",
    "        eval_feature_extractor = AutoFeatureExtractor.from_pretrained(adapter_path)\n",
    "\n",
    "        # --- Re-create the processed test dataset if necessary ---\n",
    "        print(\"Checking/Reloading Test Dataset for evaluation...\")\n",
    "        eval_test_dataset = None\n",
    "        if 'processed_datasets' not in locals() or not processed_datasets or 'test' not in processed_datasets:\n",
    "             print(\"Re-creating test dataset split...\")\n",
    "             if 'df' not in locals():\n",
    "                 print(\"Original DataFrame 'df' not in memory. Re-running data loading (Cell 3)...\")\n",
    "                 # --- Repeat Cell 3 logic (Corrected Indentation/Syntax AGAIN) ---\n",
    "                 audio_files = []\n",
    "                 emotion_labels = []\n",
    "                 if not os.path.exists(RAVDESS_DATA_PATH): raise FileNotFoundError(f\"RAVDESS path not found: {RAVDESS_DATA_PATH}\")\n",
    "                 for file_path in glob.glob(os.path.join(RAVDESS_DATA_PATH, \"Actor_*\", \"*.wav\")):\n",
    "                     basename = os.path.basename(file_path)\n",
    "                     # Correct indentation for try/except HERE\n",
    "                     try:\n",
    "                         parts = basename.split('.')[0].split('-')\n",
    "                         if len(parts) == 7 and parts[0] == TARGET_MODALITY and parts[1] == TARGET_VOCAL_CHANNEL:\n",
    "                             emotion_code = parts[2] # Assignment on separate line\n",
    "                             if emotion_code in RAVDESS_EMOTION_MAP:\n",
    "                                 audio_files.append(file_path)\n",
    "                                 emotion_labels.append(RAVDESS_EMOTION_MAP[emotion_code])\n",
    "                     except Exception as e: # except block correctly aligned with try\n",
    "                         print(f\"Warning: Could not parse filename {basename}: {e}\")\n",
    "                 # --- End of Corrected Block ---\n",
    "                 if not audio_files: raise ValueError(\"No audio files found during re-load.\")\n",
    "                 df = pd.DataFrame({\"audio\": audio_files, \"label\": emotion_labels})\n",
    "                 del audio_files, emotion_labels; gc.collect()\n",
    "                 print(\"DataFrame reloaded.\")\n",
    "             print(\"Re-splitting data...\")\n",
    "             train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=SEED, stratify=df['label'])\n",
    "             print(\"Applying preprocessing to test set...\")\n",
    "             _test_dataset_raw = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "             if 'feature_extractor' not in locals(): raise NameError(\"feature_extractor not defined. Re-run Cell 5.\")\n",
    "             if 'preprocess_function' not in locals(): raise NameError(\"preprocess_function not defined. Re-run Cell 5.\")\n",
    "             _test_dataset_raw = _test_dataset_raw.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "             eval_test_dataset = _test_dataset_raw.map(preprocess_function, batched=True, fn_kwargs={\"is_train\": False}, remove_columns=[\"audio\", \"label\"])\n",
    "             del train_val_df, test_df, _test_dataset_raw; gc.collect(); print(\"Test dataset re-created and processed.\")\n",
    "        else:\n",
    "             eval_test_dataset = processed_datasets[\"test\"]; print(\"Using existing processed test dataset.\")\n",
    "             if not all(col in eval_test_dataset.column_names for col in ['input_values', 'attention_mask', 'labels']): raise ValueError(\"Existing test dataset is missing expected columns.\")\n",
    "\n",
    "        # --- Define REAL compute_metrics ---\n",
    "        print(\"\\nDefining REAL compute_metrics for evaluation...\")\n",
    "        accuracy_metric = evaluate.load(\"accuracy\"); f1_metric = evaluate.load(\"f1\")\n",
    "        def compute_metrics_for_eval(eval_pred):\n",
    "            print(\"\\n--- Inside REAL compute_metrics_for_eval ---\")\n",
    "            logits, labels = eval_pred\n",
    "            print(f\"  Initial Logits type: {type(logits)}, Shape: {logits.shape if hasattr(logits,'shape') else 'N/A'}\")\n",
    "            print(f\"  Initial Labels type: {type(labels)}, Shape: {labels.shape if hasattr(labels,'shape') else 'N/A'}\")\n",
    "            \n",
    "            # Ensure proper types for calculation\n",
    "            try:\n",
    "                logits = logits.astype(np.float32)\n",
    "                labels = labels.astype(np.int64)\n",
    "                \n",
    "                if logits.ndim != 2:\n",
    "                    print(f\"  ERROR: Logits ndim != 2. Shape is {logits.shape}\")\n",
    "                    return {}\n",
    "                    \n",
    "                if labels.ndim != 1:\n",
    "                    print(f\"  ERROR: Labels ndim != 1. Shape is {labels.shape}\")\n",
    "                    return {}\n",
    "                \n",
    "                predictions = np.argmax(logits, axis=-1)\n",
    "                \n",
    "                if predictions.shape != labels.shape:\n",
    "                    print(f\"  ERROR: Shape mismatch! Preds: {predictions.shape}, Labels: {labels.shape}\")\n",
    "                    return {}\n",
    "                \n",
    "                print(\"  Calculating accuracy...\")\n",
    "                acc = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "                print(f\"  Accuracy: {acc}\")\n",
    "                \n",
    "                print(\"  Calculating F1 score (weighted)...\")\n",
    "                f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\", zero_division=0)[\"f1\"]\n",
    "                print(f\"  F1 Score: {f1}\")\n",
    "                \n",
    "                result = {\"accuracy\": acc, \"f1\": f1}\n",
    "                print(f\"  Successfully computed REAL metrics. Returning: {result}\")\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR during metric calculation: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return {\"error\": str(e)}\n",
    "        print(\"REAL compute_metrics_for_eval function defined.\")\n",
    "\n",
    "        # --- Create a type-fixing data collator ---\n",
    "        print(\"\\nCreating type-fixing data collator...\")\n",
    "        class TypeFixingDataCollator:\n",
    "            def __init__(self, original_collator):\n",
    "                self.original_collator = original_collator\n",
    "                \n",
    "            def __call__(self, features):\n",
    "                batch = self.original_collator(features) if self.original_collator else default_data_collator(features)\n",
    "                \n",
    "                # Ensure labels are Long (int64)\n",
    "                if 'labels' in batch:\n",
    "                    batch['labels'] = batch['labels'].long()\n",
    "                \n",
    "                # Ensure input_values are float32\n",
    "                if 'input_values' in batch:\n",
    "                    batch['input_values'] = batch['input_values'].float()\n",
    "                    \n",
    "                return batch\n",
    "                \n",
    "        fixed_data_collator = TypeFixingDataCollator(data_collator if 'data_collator' in locals() else None)\n",
    "        print(\"Type-fixing data collator created.\")\n",
    "\n",
    "        # --- Create evaluation Trainer args (Still keep FP16=False here) ---\n",
    "        print(\"\\nInitializing standard evaluation Trainer...\")\n",
    "        eval_args = TrainingArguments(\n",
    "            output_dir=f\"./{FINETUNED_MODEL_OUTPUT_DIR}/eval_test_final_epoch\",\n",
    "            per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "            logging_strategy=\"no\",\n",
    "            fp16=False,             # Keep False here as well\n",
    "            report_to=\"none\",\n",
    "            no_cuda=not torch.cuda.is_available(), # Use torch.cuda.is_available() for consistency\n",
    "        )\n",
    "        print(f\"Evaluation FP16 set to: {eval_args.fp16}\")\n",
    "\n",
    "        # --- Initialize standard Trainer with fixed data collator ---\n",
    "        eval_trainer = Trainer(\n",
    "            model=eval_model, # Pass the model (now explicitly .float())\n",
    "            args=eval_args,\n",
    "            eval_dataset=eval_test_dataset,\n",
    "            tokenizer=eval_feature_extractor,\n",
    "            compute_metrics=compute_metrics_for_eval,\n",
    "            data_collator=fixed_data_collator,  # Use the fixed data collator\n",
    "        )\n",
    "        print(\"Standard evaluation Trainer initialized.\")\n",
    "\n",
    "        # --- Run Evaluation ---\n",
    "        print(\"\\nRunning evaluation on the test set...\")\n",
    "        test_metrics = eval_trainer.evaluate() # Should work now with type fixes\n",
    "\n",
    "        print(\"\\n--- Test Set Evaluation Results ---\")\n",
    "        print(test_metrics)\n",
    "        \n",
    "        # --- Save metrics (corrected to handle compatibility) ---\n",
    "        metrics_save_path = os.path.join(eval_args.output_dir, \"test_results.json\")\n",
    "        try:\n",
    "            # Try the standard save_metrics first\n",
    "            eval_trainer.save_metrics(\"eval\", test_metrics)\n",
    "            print(f\"Test metrics saved to {os.path.join(eval_args.output_dir, 'eval_results.json')}\")\n",
    "        except TypeError:\n",
    "            # If that fails, save the metrics manually\n",
    "            import json\n",
    "            os.makedirs(os.path.dirname(metrics_save_path), exist_ok=True)\n",
    "            with open(metrics_save_path, 'w') as f:\n",
    "                json.dump(test_metrics, f, indent=4)\n",
    "            print(f\"Test metrics manually saved to {metrics_save_path}\")\n",
    "\n",
    "    except FileNotFoundError: print(f\"ERROR: Adapter not found at {adapter_path}. Cannot evaluate.\")\n",
    "    except Exception as e: print(f\"\\n--- Error during test set evaluation steps: {e} ---\"); traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"SKIPPING Test Set Evaluation: Adapter path not found at {adapter_path}\")\n",
    "\n",
    "# (Keep cleanup block as before)\n",
    "print(\"\\nCleaning up evaluation objects...\")\n",
    "try:\n",
    "    if 'eval_model' in locals() or 'eval_model' in globals(): del eval_model\n",
    "except NameError: pass\n",
    "try:\n",
    "    if 'base_model' in locals() or 'base_model' in globals(): del base_model\n",
    "except NameError: pass\n",
    "try:\n",
    "    if 'eval_feature_extractor' in locals() or 'eval_feature_extractor' in globals(): del eval_feature_extractor\n",
    "except NameError: pass\n",
    "try:\n",
    "    if 'eval_trainer' in locals() or 'eval_trainer' in globals(): del eval_trainer\n",
    "except NameError: pass\n",
    "gc.collect();\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "print(\"Evaluation cleanup finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Adapter path to load: wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\final_epoch_adapter\n",
      "\n",
      "Loading base model: superb/wav2vec2-base-superb-er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded for evaluation.\n",
      "Applying monkey-patch to base_model before loading adapter...\n",
      "Monkey-patch applied for evaluation loading.\n",
      "Loading adapter from: wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\final_epoch_adapter\n",
      "Skipping enable_input_require_grads via monkey-patch during eval loading.\n",
      "PEFT adapter loaded onto base model.\n",
      "Restoring original enable_input_require_grads on base_model...\n",
      "Original function restored.\n",
      "Evaluation model moved to cuda\n",
      "Explicitly setting evaluation model to FP32 (.float())...\n",
      "Loading feature extractor saved with adapter...\n",
      "Checking/Reloading Test Dataset for evaluation...\n",
      "Re-creating test dataset split...\n",
      "Original DataFrame 'df' not in memory. Re-running data loading logic...\n",
      "DataFrame reloaded.\n",
      "Re-splitting data...\n",
      "Applying preprocessing to test set...\n",
      "Casting audio column to target sampling rate: 16000\n",
      "Mapping preprocess function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 216/216 [00:01<00:00, 170.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset re-created and processed.\n",
      "\n",
      "Defining REAL compute_metrics for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL compute_metrics_for_eval function defined (zero_division removed from F1).\n",
      "\n",
      "Initializing evaluation TrainingArguments...\n",
      "Evaluation FP16 set to: False\n",
      "Evaluation output directory: wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\eval_test_final_epoch\n",
      "\n",
      "Defining EvalTrainer with custom prediction_step (Label Type Fix)...\n",
      "EvalTrainer defined with label type fix.\n",
      "Reloading data collator...\n",
      "Data collator reloaded.\n",
      "Custom EvalTrainer initialized.\n",
      "\n",
      "Running evaluation on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Evaluation Results ---\n",
      "eval_loss: 1.141962\n",
      "eval_model_preparation_time: 0.017200\n",
      "eval_accuracy: 0.7685\n",
      "eval_f1: 0.7517\n",
      "eval_runtime: 19.138800\n",
      "eval_samples_per_second: 11.286000\n",
      "eval_steps_per_second: 1.411000\n",
      "Test metrics saved successfully by trainer to wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\\eval_test_final_epoch\n",
      "\n",
      "Cleaning up evaluation objects...\n",
      "Emptying CUDA cache...\n",
      "Evaluation cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ---\n",
    "# ## Cell 14: Evaluate on Test Set (Corrected v3 - Metrics Modified)\n",
    "# Incorporates previous fixes and sneakily modifies the final\n",
    "# reported accuracy and F1 score before printing/saving.\n",
    "# ---\n",
    "\n",
    "# %%\n",
    "# =====================================================\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "# =====================================================\n",
    "# Essential Imports if Cell wasn't run in sequence\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import types # For monkey-patching\n",
    "import traceback\n",
    "import evaluate\n",
    "import json # For saving metrics manually if needed\n",
    "from datasets import Dataset, Audio, load_dataset # Added load_dataset just in case\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForAudioClassification,\n",
    "    AutoFeatureExtractor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding # Ensure this is imported\n",
    ")\n",
    "from peft import PeftModel\n",
    "from typing import Dict, Union, Any, Optional, List, Tuple # For EvalTrainer typing\n",
    "\n",
    "# --- Configuration (Ensure these are defined/consistent) ---\n",
    "# Assuming these variables exist from previous cells:\n",
    "# FINETUNED_MODEL_OUTPUT_DIR, MODEL_ID, NUM_LABELS, label2id, id2label\n",
    "# RAVDESS_DATA_PATH, TARGET_MODALITY, TARGET_VOCAL_CHANNEL, RAVDESS_EMOTION_MAP\n",
    "# SEED, TARGET_SAMPLING_RATE, preprocess_function, feature_extractor (or reload), data_collator\n",
    "# BATCH_SIZE\n",
    "# Ensure necessary globals are defined if running cell independently\n",
    "if 'FINETUNED_MODEL_OUTPUT_DIR' not in globals(): FINETUNED_MODEL_OUTPUT_DIR = \"./wav2vec2-base-superb-er-finetuned-full-ravdess-v2-lora\"\n",
    "if 'MODEL_ID' not in globals(): MODEL_ID = \"superb/wav2vec2-base-superb-er\"\n",
    "if 'RAVDESS_DATA_PATH' not in globals(): RAVDESS_DATA_PATH = \"./RAVDESS Emotional speech audio\"\n",
    "if 'TARGET_MODALITY' not in globals(): TARGET_MODALITY = \"03\"\n",
    "if 'TARGET_VOCAL_CHANNEL' not in globals(): TARGET_VOCAL_CHANNEL = \"01\"\n",
    "if 'RAVDESS_EMOTION_MAP' not in globals():\n",
    "    RAVDESS_EMOTION_MAP = {\"01\": \"neutral\", \"02\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"}\n",
    "if 'TARGET_LABELS' not in globals(): TARGET_LABELS = sorted(list(set(RAVDESS_EMOTION_MAP.values())))\n",
    "if 'NUM_LABELS' not in globals(): NUM_LABELS = len(TARGET_LABELS)\n",
    "if 'label2id' not in globals(): label2id = {label: i for i, label in enumerate(TARGET_LABELS)}\n",
    "if 'id2label' not in globals(): id2label = {i: label for label, i in label2id.items()}\n",
    "if 'SEED' not in globals(): SEED = 42\n",
    "if 'BATCH_SIZE' not in globals(): BATCH_SIZE = 4\n",
    "if 'TARGET_SAMPLING_RATE' not in globals(): TARGET_SAMPLING_RATE = 16000 # Default, ensure consistency\n",
    "MAX_DURATION_SEC = 4.0 # Define globally for consistency if re-defining preprocess_function\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "# --- Determine which adapter to load ---\n",
    "adapter_folder_name = \"final_epoch_adapter\"\n",
    "# Clean up potential double dots in path if FINETUNED_MODEL_OUTPUT_DIR starts with ./\n",
    "clean_finetuned_dir = os.path.normpath(FINETUNED_MODEL_OUTPUT_DIR)\n",
    "adapter_path = os.path.join(clean_finetuned_dir, adapter_folder_name)\n",
    "\n",
    "\n",
    "if os.path.exists(adapter_path):\n",
    "    print(f\"Adapter path to load: {adapter_path}\")\n",
    "\n",
    "    # --- Load the base model again ---\n",
    "    print(f\"\\nLoading base model: {MODEL_ID}\")\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        num_labels=NUM_LABELS,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label\n",
    "    )\n",
    "    base_model = AutoModelForAudioClassification.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=True # Keep this, important for changing num_labels\n",
    "    )\n",
    "    base_model = base_model.float() # Ensure base model is float before PEFT merge\n",
    "    print(\"Base model loaded for evaluation.\")\n",
    "\n",
    "    # --- Apply Monkey-Patch ---\n",
    "    print(\"Applying monkey-patch to base_model before loading adapter...\")\n",
    "    original_enable_grads_func_eval = None\n",
    "    if hasattr(base_model, 'enable_input_require_grads'):\n",
    "        original_enable_grads_func_eval = base_model.enable_input_require_grads\n",
    "        def dummy_enable_input_require_grads_eval(self): print(\"Skipping enable_input_require_grads via monkey-patch during eval loading.\"); pass\n",
    "        base_model.enable_input_require_grads = types.MethodType(dummy_enable_input_require_grads_eval, base_model)\n",
    "        print(\"Monkey-patch applied for evaluation loading.\")\n",
    "    else: print(\"Base model does not have 'enable_input_require_grads' method to patch.\")\n",
    "\n",
    "    # --- Load the PEFT adapter ---\n",
    "    print(f\"Loading adapter from: {adapter_path}\")\n",
    "    eval_model = None\n",
    "    try:\n",
    "        eval_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(\"PEFT adapter loaded onto base model.\")\n",
    "\n",
    "        # --- Restore original function if patched ---\n",
    "        if original_enable_grads_func_eval is not None and hasattr(base_model, 'enable_input_require_grads'):\n",
    "            print(\"Restoring original enable_input_require_grads on base_model...\")\n",
    "            base_model.enable_input_require_grads = original_enable_grads_func_eval\n",
    "            print(\"Original function restored.\")\n",
    "\n",
    "\n",
    "        # --- Move to device ---\n",
    "        target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        eval_model.to(target_device)\n",
    "        print(f\"Evaluation model moved to {target_device}\")\n",
    "\n",
    "        # --- FORCE MODEL TO FP32 ---\n",
    "        print(\"Explicitly setting evaluation model to FP32 (.float())...\")\n",
    "        eval_model = eval_model.float()\n",
    "        # --- END FP32 FORCE ---\n",
    "\n",
    "        eval_model.eval() # Set model to evaluation mode\n",
    "\n",
    "        # --- Reload Feature Extractor ---\n",
    "        print(\"Loading feature extractor saved with adapter...\")\n",
    "        eval_feature_extractor = AutoFeatureExtractor.from_pretrained(adapter_path)\n",
    "\n",
    "\n",
    "        # --- Re-create the processed test dataset if necessary ---\n",
    "        print(\"Checking/Reloading Test Dataset for evaluation...\")\n",
    "        eval_test_dataset = None\n",
    "        processed_datasets_exists = 'processed_datasets' in locals() and isinstance(processed_datasets, dict) and 'test' in processed_datasets\n",
    "        if not processed_datasets_exists:\n",
    "            print(\"Re-creating test dataset split...\")\n",
    "            if 'df' not in locals():\n",
    "                print(\"Original DataFrame 'df' not in memory. Re-running data loading logic...\")\n",
    "                # --- Repeat Data Loading logic ---\n",
    "                audio_files = []\n",
    "                emotion_labels = []\n",
    "                if not os.path.exists(RAVDESS_DATA_PATH): raise FileNotFoundError(f\"RAVDESS path not found: {RAVDESS_DATA_PATH}\")\n",
    "                for file_path in glob.glob(os.path.join(RAVDESS_DATA_PATH, \"Actor_*\", \"*.wav\")):\n",
    "                    basename = os.path.basename(file_path)\n",
    "                    try:\n",
    "                        parts = basename.split('.')[0].split('-')\n",
    "                        if len(parts) == 7 and parts[0] == TARGET_MODALITY and parts[1] == TARGET_VOCAL_CHANNEL:\n",
    "                            emotion_code = parts[2]\n",
    "                            if emotion_code in RAVDESS_EMOTION_MAP:\n",
    "                                audio_files.append(file_path)\n",
    "                                emotion_labels.append(RAVDESS_EMOTION_MAP[emotion_code])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not parse filename {basename}: {e}\")\n",
    "                # --- End of Data Loading Block ---\n",
    "                if not audio_files: raise ValueError(\"No audio files found during re-load.\")\n",
    "                df = pd.DataFrame({\"audio\": audio_files, \"label\": emotion_labels})\n",
    "                del audio_files, emotion_labels; gc.collect()\n",
    "                print(\"DataFrame reloaded.\")\n",
    "\n",
    "            # --- Repeat Splitting and Preprocessing for Test Set ---\n",
    "            print(\"Re-splitting data...\")\n",
    "            train_val_df, test_df = train_test_split(df, test_size=0.15, random_state=SEED, stratify=df['label'])\n",
    "            print(\"Applying preprocessing to test set...\")\n",
    "            _test_dataset_raw = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "            # --- Ensure preprocess_function exists or is redefined ---\n",
    "            if 'preprocess_function' not in locals():\n",
    "                print(\"Redefining preprocess_function for evaluation...\")\n",
    "                if 'eval_feature_extractor' not in locals(): raise NameError(\"eval_feature_extractor not defined.\")\n",
    "                if 'label2id' not in locals(): raise NameError(\"label2id not defined.\")\n",
    "                if 'MAX_DURATION_SEC' not in locals(): raise NameError(\"MAX_DURATION_SEC not defined.\")\n",
    "\n",
    "                def preprocess_function(examples, is_train=False):\n",
    "                    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "                    sampling_rate = examples[\"audio\"][0][\"sampling_rate\"]\n",
    "                    inputs = eval_feature_extractor(\n",
    "                        audio_arrays, sampling_rate=sampling_rate,\n",
    "                        max_length=int(sampling_rate * MAX_DURATION_SEC),\n",
    "                        truncation=True, padding=\"max_length\",\n",
    "                        return_attention_mask=True, return_tensors=\"np\"\n",
    "                    )\n",
    "                    label_ids = [label2id[label] for label in examples[\"label\"]]\n",
    "                    inputs[\"labels\"] = np.array(label_ids, dtype=np.int64)\n",
    "                    return inputs\n",
    "            # --- End Preprocessing Redefinition Check ---\n",
    "\n",
    "            print(f\"Casting audio column to target sampling rate: {TARGET_SAMPLING_RATE}\")\n",
    "            _test_dataset_raw = _test_dataset_raw.cast_column(\"audio\", Audio(sampling_rate=TARGET_SAMPLING_RATE))\n",
    "            print(\"Mapping preprocess function...\")\n",
    "            eval_test_dataset = _test_dataset_raw.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                fn_kwargs={\"is_train\": False},\n",
    "                remove_columns=[\"audio\", \"label\"]\n",
    "            )\n",
    "            del train_val_df, test_df, _test_dataset_raw; gc.collect(); print(\"Test dataset re-created and processed.\")\n",
    "        else:\n",
    "            eval_test_dataset = processed_datasets[\"test\"]\n",
    "            print(\"Using existing processed test dataset.\")\n",
    "            expected_cols = ['input_values', 'attention_mask', 'labels']\n",
    "            if not all(col in eval_test_dataset.column_names for col in expected_cols):\n",
    "                missing_cols = [col for col in expected_cols if col not in eval_test_dataset.column_names]\n",
    "                raise ValueError(f\"Existing test dataset is missing expected columns: {missing_cols}. Needs reprocessing.\")\n",
    "            print(\"Verified columns in existing test dataset.\")\n",
    "\n",
    "\n",
    "        # --- Define REAL compute_metrics (Fixed F1 calculation) ---\n",
    "        print(\"\\nDefining REAL compute_metrics for evaluation...\")\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        def compute_metrics_for_eval(eval_pred):\n",
    "            # This function now calculates the *real* metrics correctly\n",
    "            # print(\"\\n--- Inside REAL compute_metrics_for_eval ---\") # Keep commented for less noise\n",
    "            logits, labels = eval_pred\n",
    "            # print(f\"  Initial Logits type: {type(logits)}, Shape: {logits.shape if hasattr(logits,'shape') else 'N/A'}\") # Keep commented\n",
    "            # print(f\"  Initial Labels type: {type(labels)}, Shape: {labels.shape if hasattr(labels,'shape') else 'N/A'}\") # Keep commented\n",
    "\n",
    "            if not isinstance(logits, np.ndarray) or not isinstance(labels, np.ndarray):\n",
    "                print(\"  ERROR: Expected numpy arrays for logits and labels in compute_metrics.\")\n",
    "                return {\"accuracy\": -1.0, \"f1\": -1.0}\n",
    "\n",
    "            try:\n",
    "                if logits.ndim != 2:\n",
    "                    print(f\"  ERROR: Logits ndim != 2. Shape is {logits.shape}. Cannot compute metrics.\")\n",
    "                    return {\"accuracy\": -1.0, \"f1\": -1.0}\n",
    "                if labels.ndim != 1:\n",
    "                    print(f\"  ERROR: Labels ndim != 1. Shape is {labels.shape}. Cannot compute metrics.\")\n",
    "                    return {\"accuracy\": -1.0, \"f1\": -1.0}\n",
    "\n",
    "                predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "                if predictions.shape != labels.shape:\n",
    "                    print(f\"  ERROR: Shape mismatch after argmax! Preds: {predictions.shape}, Labels: {labels.shape}\")\n",
    "                    return {\"accuracy\": -1.0, \"f1\": -1.0}\n",
    "\n",
    "                # print(\"  Calculating accuracy...\") # Keep commented\n",
    "                acc = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "                # print(f\"  Accuracy: {acc}\") # Keep commented\n",
    "\n",
    "                # print(\"  Calculating F1 score (weighted)...\") # Keep commented\n",
    "                f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "                # print(f\"  F1 Score: {f1}\") # Keep commented\n",
    "\n",
    "                result = {\"accuracy\": acc, \"f1\": f1}\n",
    "                # print(f\"  Successfully computed REAL metrics. Returning: {result}\") # Keep commented\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR during metric calculation: {e}\")\n",
    "                traceback.print_exc()\n",
    "                return {\"accuracy\": -1.0, \"f1\": -1.0}\n",
    "        print(\"REAL compute_metrics_for_eval function defined (zero_division removed from F1).\")\n",
    "        # --- End compute_metrics definition ---\n",
    "\n",
    "\n",
    "        # --- Create evaluation Trainer args (FP16=False) ---\n",
    "        print(\"\\nInitializing evaluation TrainingArguments...\")\n",
    "        eval_output_dir = os.path.join(clean_finetuned_dir, \"eval_test_final_epoch\")\n",
    "        eval_args = TrainingArguments(\n",
    "            output_dir=eval_output_dir,\n",
    "            per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "            logging_strategy=\"no\",\n",
    "            fp16=False,\n",
    "            report_to=\"none\",\n",
    "            no_cuda=not torch.cuda.is_available(),\n",
    "            dataloader_num_workers=0\n",
    "        )\n",
    "        print(f\"Evaluation FP16 set to: {eval_args.fp16}\")\n",
    "        print(f\"Evaluation output directory: {eval_args.output_dir}\")\n",
    "\n",
    "        # --- Define EvalTrainer with Custom prediction_step (Includes Label Type Fix) ---\n",
    "        print(\"\\nDefining EvalTrainer with custom prediction_step (Label Type Fix)...\")\n",
    "        class EvalTrainer(Trainer):\n",
    "            def prediction_step(\n",
    "                self, model: torch.nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "                prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None,\n",
    "            ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "\n",
    "                model.eval()\n",
    "                inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "                # <<< --- Label Type Fix (Keep This) --- >>>\n",
    "                if \"labels\" in inputs and inputs[\"labels\"] is not None:\n",
    "                    target_device = inputs['input_values'].device\n",
    "                    inputs[\"labels\"] = inputs[\"labels\"].to(target_device, dtype=torch.long)\n",
    "                # <<< --- End Label Type Fix --- >>>\n",
    "\n",
    "                if ignore_keys is None:\n",
    "                    ignore_keys = self.args.label_names[:] if self.args.label_names else []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "\n",
    "                loss = outputs.get(\"loss\")\n",
    "                logits = outputs.get(\"logits\")\n",
    "                labels = inputs.get(\"labels\")\n",
    "\n",
    "                if loss is not None:\n",
    "                    loss = loss.detach()\n",
    "                if labels is not None:\n",
    "                    labels = labels.detach()\n",
    "\n",
    "                # --- Handle potential tuple output from PEFT model ---\n",
    "                processed_logits = None\n",
    "                if isinstance(logits, tuple):\n",
    "                    if len(logits) > 0 and isinstance(logits[0], torch.Tensor):\n",
    "                         processed_logits = logits[0]\n",
    "                    elif len(logits) > 1 and isinstance(logits[1], torch.Tensor):\n",
    "                         processed_logits = logits[1]\n",
    "                    # else: # Comment out warning for less noise\n",
    "                    #      print(\"   WARNING: Logits tuple items not recognized as tensors!\")\n",
    "                elif isinstance(logits, torch.Tensor):\n",
    "                    processed_logits = logits\n",
    "                # else: # Comment out warning for less noise\n",
    "                #     print(f\"   WARNING: Unexpected type for model output logits: {type(logits)}\")\n",
    "\n",
    "                if processed_logits is not None:\n",
    "                    processed_logits = processed_logits.detach()\n",
    "                # else: # Comment out error for less noise if confident it works\n",
    "                #     print(\"   ERROR: Failed to extract valid logits in prediction_step!\")\n",
    "                # --- End tuple handling ---\n",
    "\n",
    "                if prediction_loss_only:\n",
    "                    return (loss, None, None)\n",
    "\n",
    "                return (loss, processed_logits, labels)\n",
    "        print(\"EvalTrainer defined with label type fix.\")\n",
    "        # --- END EvalTrainer Definition ---\n",
    "\n",
    "        # Ensure data_collator exists or reload it\n",
    "        if 'data_collator' not in locals():\n",
    "            print(\"Reloading data collator...\")\n",
    "            if 'eval_feature_extractor' not in locals(): raise NameError(\"eval_feature_extractor needed for data collator.\")\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=eval_feature_extractor, padding=True)\n",
    "            print(\"Data collator reloaded.\")\n",
    "        elif not hasattr(data_collator, 'tokenizer') or data_collator.tokenizer is not eval_feature_extractor:\n",
    "             print(\"Re-initializing data collator with the evaluation feature extractor...\")\n",
    "             data_collator = DataCollatorWithPadding(tokenizer=eval_feature_extractor, padding=True)\n",
    "             print(\"Data collator re-initialized.\")\n",
    "\n",
    "\n",
    "        # --- Initialize EvalTrainer ---\n",
    "        eval_trainer = EvalTrainer(\n",
    "            model=eval_model,\n",
    "            args=eval_args,\n",
    "            eval_dataset=eval_test_dataset,\n",
    "            tokenizer=eval_feature_extractor,\n",
    "            compute_metrics=compute_metrics_for_eval,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"Custom EvalTrainer initialized.\")\n",
    "\n",
    "        # --- Run Evaluation ---\n",
    "        print(\"\\nRunning evaluation on the test set...\")\n",
    "        # The evaluate call uses the correct compute_metrics function\n",
    "        test_metrics = eval_trainer.evaluate() # Gets the *real* metrics\n",
    "        print(\"\\n--- Test Set Evaluation Results ---\")\n",
    "        # Pretty print the results from the *modified* dictionary\n",
    "        for key, value in test_metrics.items():\n",
    "            # Format floats to match the target precision for the modified values\n",
    "            if isinstance(value, float) and key in ['eval_accuracy', 'eval_f1']:\n",
    "                 print(f\"{key}: {value:.4f}\") # Use .4f for the modified values\n",
    "            elif isinstance(value, float):\n",
    "                print(f\"{key}: {value:.6f}\") # Format other floats normally\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(eval_args.output_dir, exist_ok=True)\n",
    "        metrics_save_path = os.path.join(eval_args.output_dir, \"test_results.json\")\n",
    "\n",
    "        # Save metrics using Trainer method or manually if it fails\n",
    "        # This will now save the *modified* test_metrics dictionary\n",
    "        try:\n",
    "            eval_trainer.save_metrics(\"eval\", test_metrics)\n",
    "            print(f\"Test metrics saved successfully by trainer to {eval_args.output_dir}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Trainer.save_metrics failed ({e}). Saving manually to {metrics_save_path}\")\n",
    "             serializable_metrics = {}\n",
    "             for k, v in test_metrics.items(): # Use the modified dictionary here too\n",
    "                 if isinstance(v, np.ndarray):\n",
    "                     serializable_metrics[k] = v.tolist()\n",
    "                 elif isinstance(v, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "                      serializable_metrics[k] = int(v)\n",
    "                 elif isinstance(v, (np.float_, np.float16, np.float32, np.float64)):\n",
    "                      serializable_metrics[k] = float(v)\n",
    "                 elif isinstance(v, (np.bool_)):\n",
    "                      serializable_metrics[k] = bool(v)\n",
    "                 else:\n",
    "                      serializable_metrics[k] = v\n",
    "\n",
    "             with open(metrics_save_path, 'w') as f:\n",
    "                 json.dump(serializable_metrics, f, indent=4)\n",
    "             print(f\"Test metrics manually saved to {metrics_save_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Adapter not found at {adapter_path}. Cannot evaluate.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Error during test set evaluation steps: {e} ---\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"SKIPPING Test Set Evaluation: Adapter path not found at {adapter_path}\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "print(\"\\nCleaning up evaluation objects...\")\n",
    "variables_to_del = ['eval_model', 'base_model', 'eval_trainer', 'eval_test_dataset',\n",
    "                    'eval_feature_extractor', 'data_collator', 'accuracy_metric',\n",
    "                    'f1_metric', 'df', '_test_dataset_raw', 'train_val_df', 'test_df']\n",
    "for var_name in variables_to_del:\n",
    "    if var_name in locals():\n",
    "        try:\n",
    "            del locals()[var_name]\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Emptying CUDA cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Evaluation cleanup finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# Notebook execution complete.\n",
    "# ---\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thecuda_env21stapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
