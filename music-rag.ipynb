{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T18:28:57.087177Z",
     "iopub.status.busy": "2025-03-10T18:28:57.086784Z",
     "iopub.status.idle": "2025-03-10T18:29:00.571757Z",
     "shell.execute_reply": "2025-03-10T18:29:00.570635Z",
     "shell.execute_reply.started": "2025-03-10T18:28:57.087146Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert CSV into NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T18:29:03.052567Z",
     "iopub.status.busy": "2025-03-10T18:29:03.052192Z",
     "iopub.status.idle": "2025-03-10T18:29:06.504349Z",
     "shell.execute_reply": "2025-03-10T18:29:06.503476Z",
     "shell.execute_reply.started": "2025-03-10T18:29:03.052538Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and process csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T18:44:35.169938Z",
     "iopub.status.busy": "2025-03-10T18:44:35.169533Z",
     "iopub.status.idle": "2025-03-10T18:57:31.717514Z",
     "shell.execute_reply": "2025-03-10T18:57:31.716581Z",
     "shell.execute_reply.started": "2025-03-10T18:44:35.169904Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"/kaggle/input/tagged-music-tokens/tagged_music_data.csv\")  # Adjust path\n",
    "\n",
    "# Initialize Hugging Face Sentence Transformers\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = len(embeddings.embed_query(\" \"))\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# ‚úÖ Process in Batches to Reduce Memory Usage\n",
    "batch_size = 500  # Adjust based on available memory\n",
    "music_data = df.to_dict(orient=\"records\")  # Convert rows to dictionaries\n",
    "texts = [str(row) for row in music_data]  # Convert each row to a string\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i : i + batch_size]  # Process batch-wise\n",
    "    vector_store.add_texts(batch)\n",
    "    print(f\"‚úÖ Processed {i + batch_size} records\")\n",
    "\n",
    "print(\"‚úÖ FAISS Vector Store Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent for Data-to-text Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T19:06:53.799035Z",
     "iopub.status.busy": "2025-03-10T19:06:53.798658Z",
     "iopub.status.idle": "2025-03-10T19:06:59.863684Z",
     "shell.execute_reply": "2025-03-10T19:06:59.862831Z",
     "shell.execute_reply.started": "2025-03-10T19:06:53.799008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# üîπ Load LLaMA 1B Model Correctly\n",
    "HF_TOKEN = \"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\"\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"meta-llama/Llama-3.2-1B\", \n",
    "    device_map=\"auto\", \n",
    "    token=HF_TOKEN  # ‚úÖ Correct way to pass token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T19:13:04.053019Z",
     "iopub.status.busy": "2025-03-10T19:13:04.052606Z",
     "iopub.status.idle": "2025-03-10T19:13:18.155780Z",
     "shell.execute_reply": "2025-03-10T19:13:18.155072Z",
     "shell.execute_reply.started": "2025-03-10T19:13:04.052988Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T19:07:08.843128Z",
     "iopub.status.busy": "2025-03-10T19:07:08.842768Z",
     "iopub.status.idle": "2025-03-10T19:07:19.321580Z",
     "shell.execute_reply": "2025-03-10T19:07:19.320560Z",
     "shell.execute_reply.started": "2025-03-10T19:07:08.843102Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# üöÄ **Agent: Convert Music Data to Text Representation**\n",
    "def generate_music_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Uses FAISS to retrieve similar music tokens and converts them into a natural language music description.\n",
    "    \"\"\"\n",
    "    # üîç Retrieve similar stored music descriptions\n",
    "    retrieved_docs = vector_store.similarity_search(user_input, k=3)\n",
    "\n",
    "    # ‚úÖ Extract text properly\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs if hasattr(doc, \"page_content\")]\n",
    "\n",
    "    if not retrieved_texts:\n",
    "        return \"No relevant music data found.\"\n",
    "\n",
    "    # üéº Format prompt for LLaMA\n",
    "    prompt = (\n",
    "        \"You are a music expert that converts structured music token data (pitch, duration, velocity, emotion) \"\n",
    "        \"into a natural language description. Convert the following retrieved data into a meaningful text prompt \"\n",
    "        \"for a text-to-music model:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\"\n",
    "        \"Describe the music in terms of mood, tempo, and instruments used.\"\n",
    "    )\n",
    "\n",
    "    # üìù Generate response using LLaMA\n",
    "    response = llama_pipeline(prompt, max_length=256)[0][\"generated_text\"]\n",
    "    return response  # üé∂ Return the formatted text prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T19:15:12.736995Z",
     "iopub.status.busy": "2025-03-10T19:15:12.736641Z",
     "iopub.status.idle": "2025-03-10T19:15:12.742225Z",
     "shell.execute_reply": "2025-03-10T19:15:12.741238Z",
     "shell.execute_reply.started": "2025-03-10T19:15:12.736971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_music_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Uses FAISS to retrieve similar music tokens and applies Chain of Thought (CoT) reasoning\n",
    "    to generate a detailed and structured music prompt.\n",
    "    \"\"\"\n",
    "    # üîç **Step 1: Retrieve Relevant Musical Data**\n",
    "    retrieved_docs = vector_store.similarity_search(user_input, k=3)\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs if hasattr(doc, \"page_content\")]\n",
    "\n",
    "    if not retrieved_texts:\n",
    "        return \"No relevant music data found.\"\n",
    "\n",
    "    # üß† **Step 2: Reasoning about the Music Style**\n",
    "    reasoning_steps = (\n",
    "        \"**Step 1: Identify Mood and Emotion**\\n\"\n",
    "        \"- Extract the mood (e.g., calm, energetic, melancholic) from the retrieved data.\\n\\n\"\n",
    "        \"**Step 2: Identify Tempo and Dynamics**\\n\"\n",
    "        \"- Determine if the tempo is slow, moderate, or fast.\\n\"\n",
    "        \"- Identify dynamics like soft (piano) or loud (forte).\\n\\n\"\n",
    "        \"**Step 3: Identify Instrumentation**\\n\"\n",
    "        \"- Determine which instruments dominate (e.g., piano, violin, guitar).\\n\\n\"\n",
    "        \"**Step 4: Structure a Final Music Prompt**\\n\"\n",
    "        \"- Construct a natural language music description incorporating the above features.\\n\"\n",
    "    )\n",
    "\n",
    "    # üìù **Step 3: Format Prompt for LLaMA**\n",
    "    prompt = (\n",
    "        \"You are a music expert that converts structured music token data (pitch, duration, velocity, emotion) \"\n",
    "        \"into a natural language description.\\n\\n\"\n",
    "        \"Follow these steps:\\n\"\n",
    "        f\"{reasoning_steps}\\n\"\n",
    "        \"Now, based on the following retrieved data, generate a structured and meaningful text prompt for a text-to-music model:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\"\n",
    "        \"Describe the music in terms of mood, tempo, and instruments used.\"\n",
    "    )\n",
    "\n",
    "    # üéº **Step 4: Generate Structured Response using LLaMA**\n",
    "    response = llama_pipeline(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
    "\n",
    "    return response  # üé∂ Return the refined music prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T19:21:13.477589Z",
     "iopub.status.busy": "2025-03-10T19:21:13.477254Z",
     "iopub.status.idle": "2025-03-10T19:21:13.483198Z",
     "shell.execute_reply": "2025-03-10T19:21:13.482198Z",
     "shell.execute_reply.started": "2025-03-10T19:21:13.477564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_music_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Uses FAISS to retrieve similar music tokens and applies Tree of Thought (ToT) reasoning\n",
    "    to generate a detailed and structured music prompt.\n",
    "    \"\"\"\n",
    "    # üîç **Step 1: Retrieve Relevant Musical Data**\n",
    "    retrieved_docs = vector_store.similarity_search(user_input, k=50)\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs if hasattr(doc, \"page_content\")]\n",
    "\n",
    "    if not retrieved_texts:\n",
    "        return \"No relevant music data found.\"\n",
    "\n",
    "    # üå≤ **Step 2: Tree of Thought Reasoning**\n",
    "    tree_of_thought = (\n",
    "        \"**Branch 1: Musical Emotion Analysis**\\n\"\n",
    "        \"- What emotions are commonly associated with the retrieved music tokens?\\n\"\n",
    "        \"- How do different elements (chords, tempo, dynamics) contribute to the mood?\\n\\n\"\n",
    "\n",
    "        \"**Branch 2: Tempo & Rhythm Structure**\\n\"\n",
    "        \"- What is the general tempo (BPM range) of the retrieved examples?\\n\"\n",
    "        \"- Are there noticeable rhythm patterns (e.g., waltz, syncopation)?\\n\\n\"\n",
    "\n",
    "        \"**Branch 3: Instrumentation Breakdown**\\n\"\n",
    "        \"- What instruments are used in the retrieved samples?\\n\"\n",
    "        \"- How does instrumentation impact the texture and feel of the music?\\n\\n\"\n",
    "\n",
    "        \"**Branch 4: Genre & Style Refinement**\\n\"\n",
    "        \"- What genre does this music best fit into?\\n\"\n",
    "        \"- Are there any unique stylistic elements that stand out?\\n\\n\"\n",
    "\n",
    "        \"**Branch 5: Final Music Prompt Construction**\\n\"\n",
    "        \"- Combine insights from the previous branches into a well-structured, natural language prompt.\\n\"\n",
    "        \"- Ensure clarity in describing mood, tempo, instrumentation, and genre.\"\n",
    "    )\n",
    "\n",
    "    # üìù **Step 3: Format Prompt for LLaMA**\n",
    "    prompt = (\n",
    "        \"You are a music expert skilled in analyzing structured music tokens (pitch, duration, velocity, emotion) \"\n",
    "        \"and transforming them into a rich, human-readable music description.\\n\\n\"\n",
    "        \"Follow this Tree of Thought framework to break down the musical elements before generating a response:\\n\\n\"\n",
    "        f\"{tree_of_thought}\\n\"\n",
    "        \"Now, using the following retrieved data, generate a well-structured and insightful text prompt:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\"\n",
    "        \"Ensure that the final output describes the music in terms of **emotion, tempo, rhythm, instruments, and style**.\"\n",
    "    )\n",
    "\n",
    "    # üéº **Step 4: Generate Structured Response using LLaMA**\n",
    "    response = llama_pipeline(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
    "\n",
    "    return response  # üé∂ Return the refined music prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T19:24:46.744843Z",
     "iopub.status.busy": "2025-03-10T19:24:46.744450Z",
     "iopub.status.idle": "2025-03-10T19:24:46.750796Z",
     "shell.execute_reply": "2025-03-10T19:24:46.749695Z",
     "shell.execute_reply.started": "2025-03-10T19:24:46.744803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_music_prompt(user_input):\n",
    "    \"\"\"\n",
    "    Uses FAISS to retrieve similar music tokens and applies Graph of Thought (GoT) reasoning\n",
    "    to generate a detailed and structured music prompt.\n",
    "    \"\"\"\n",
    "    # üîç **Step 1: Retrieve Relevant Musical Data**\n",
    "    retrieved_docs = vector_store.similarity_search(user_input, k=60)\n",
    "    retrieved_texts = [doc.page_content for doc in retrieved_docs if hasattr(doc, \"page_content\")]\n",
    "\n",
    "    if not retrieved_texts:\n",
    "        return \"No relevant music data found.\"\n",
    "\n",
    "    # üîó **Step 2: Graph of Thought Reasoning**\n",
    "    graph_of_thought = (\n",
    "        \"**Nodes (Core Elements):**\\n\"\n",
    "        \"- üéµ Mood & Emotion (happy, sad, energetic, calm, etc.)\\n\"\n",
    "        \"- üéº Tempo & Rhythm (BPM, patterns, time signature)\\n\"\n",
    "        \"- üéª Instrumentation (piano, violin, guitar, etc.)\\n\"\n",
    "        \"- üé∑ Genre & Style (jazz, classical, electronic, etc.)\\n\\n\"\n",
    "        \n",
    "        \"**Edges (Interconnections Between Elements):**\\n\"\n",
    "        \"- How does **tempo** influence the **mood**? (e.g., fast tempo = energetic, slow tempo = relaxing)\\n\"\n",
    "        \"- How does **instrumentation** shape the **genre**? (e.g., strings = classical, synths = electronic)\\n\"\n",
    "        \"- How do **rhythm patterns** contribute to **emotion**? (e.g., syncopation in jazz for swing feel)\\n\"\n",
    "        \"- How do **dynamic variations** affect **style**? (e.g., crescendo for intensity, legato for smoothness)\\n\\n\"\n",
    "\n",
    "        \"**Graph-based Reasoning Steps:**\\n\"\n",
    "        \"1Ô∏è‚É£ Identify the most relevant nodes (e.g., if the user wants 'relaxing piano melody', focus on **mood**, **tempo**, and **instrumentation**).\\n\"\n",
    "        \"2Ô∏è‚É£ Find edges (connections) that define relationships (e.g., 'slow tempo' + 'soft piano' ‚Üí 'calm, meditative atmosphere').\\n\"\n",
    "        \"3Ô∏è‚É£ Generate a natural language description integrating all relevant nodes and their relationships.\\n\"\n",
    "    )\n",
    "\n",
    "    # üìù **Step 3: Format Prompt for LLaMA**\n",
    "    prompt = (\n",
    "        \"You are an expert in music generation who transforms structured music tokens (pitch, duration, velocity, emotion) \"\n",
    "        \"into rich, expressive descriptions.\\n\\n\"\n",
    "        \"Use the **Graph of Thought** framework to analyze musical relationships before generating a structured prompt:\\n\\n\"\n",
    "        f\"{graph_of_thought}\\n\"\n",
    "        \"Now, based on the following retrieved data, generate a well-structured and interconnected music description:\\n\\n\"\n",
    "        f\"{retrieved_texts}\\n\"\n",
    "        \"Ensure the output describes **mood, tempo, instrumentation, rhythm, and their interconnections**.\"\n",
    "    )\n",
    "\n",
    "    # üéº **Step 4: Generate Structured Response using LLaMA**\n",
    "    response = llama_pipeline(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
    "\n",
    "    return response  # üé∂ Return the refined music prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T19:25:25.117034Z",
     "iopub.status.busy": "2025-03-10T19:25:25.116696Z",
     "iopub.status.idle": "2025-03-10T19:25:36.117642Z",
     "shell.execute_reply": "2025-03-10T19:25:36.116718Z",
     "shell.execute_reply.started": "2025-03-10T19:25:25.117011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "# üîπ **Example User Query**\n",
    "user_query = \"Generate an happy piano melody\"\n",
    "\n",
    "# üé∂ Generate refined prompt\n",
    "final_music_prompt = generate_music_prompt(user_query)\n",
    "print(\"Generated Prompt:\", final_music_prompt)\n",
    "\n",
    "# üéµ **Generate Music using MusicGen**\n",
    "inputs = processor(text=[final_music_prompt], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "audio_values = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "# üéº **Convert to WAV and Save**\n",
    "sample_rate = 16000  # Set sample rate\n",
    "audio_array = audio_values.cpu().detach().numpy().squeeze()  # Convert tensor to numpy\n",
    "write(\"generated_music.wav\", sample_rate, audio_array)  # ‚úÖ Now `write` is correctly imported and used\n",
    "\n",
    "print(\"‚úÖ Music saved as 'generated_music.wav' üé∂\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6815026,
     "sourceId": 10954973,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
